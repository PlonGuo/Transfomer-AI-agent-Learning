# ä»Transformeråˆ°AI Agentå®Œæ•´å­¦ä¹ æŒ‡å—

> ä¸ºJasonå®šåˆ¶ - ä»åº•å±‚åŸç†åˆ°å®æˆ˜åº”ç”¨çš„13å‘¨å­¦ä¹ è·¯çº¿
> 
> æœ€åæ›´æ–°: 2024å¹´12æœˆ

---

## ğŸ“– ç›®å½•

1. [PyTorchå‰ç½®è¦æ±‚ - é‡è¦å¿…è¯»](#pytorchå‰ç½®è¦æ±‚---é‡è¦å¿…è¯»)
2. [å­¦ä¹ è·¯çº¿å›¾](#å­¦ä¹ è·¯çº¿å›¾)
3. [Week 0: PyTorchåŸºç¡€é€Ÿæˆ (1å‘¨)](#week-0-pytorchåŸºç¡€é€Ÿæˆ)
4. [Level 1: TransformeråŸºç¡€ (2-3å‘¨)](#level-1-transformeråŸºç¡€)
5. [Level 2: LLMå·¥ä½œåŸç† (1-2å‘¨)](#level-2-llmå·¥ä½œåŸç†)
6. [Level 3: Prompt Engineering & RAG (1å‘¨)](#level-3-prompt-engineering--rag)
7. [Level 4: AI Agentæ¶æ„ (2-3å‘¨)](#level-4-ai-agentæ¶æ„)
8. [Level 5: å®æˆ˜é¡¹ç›® (æŒç»­)](#level-5-å®æˆ˜é¡¹ç›®)
9. [13å‘¨è¯¦ç»†å­¦ä¹ è®¡åˆ’](#13å‘¨è¯¦ç»†å­¦ä¹ è®¡åˆ’)
10. [é¢å¤–èµ„æº](#é¢å¤–èµ„æº)

---

## âš ï¸ PyTorchå‰ç½®è¦æ±‚ - é‡è¦å¿…è¯»

### ä¸ºä»€ä¹ˆéœ€è¦PyTorch?

æœ¬å­¦ä¹ è®¡åˆ’ä¸­ **Level 1-2 (Week 1-5)** å¤§é‡ä½¿ç”¨PyTorch:

```
éœ€è¦PyTorchçš„éƒ¨åˆ†:
â”œâ”€â”€ âœ… Level 1: æ‰‹æ’¸Transformer (é‡åº¦ä½¿ç”¨)
â”‚   â”œâ”€â”€ nanoGPTæºç é˜…è¯»
â”‚   â”œâ”€â”€ å®ç°attentionæœºåˆ¶
â”‚   â””â”€â”€ è®­ç»ƒminiè¯­è¨€æ¨¡å‹
â”œâ”€â”€ âœ… Level 2: Fine-tuningæ¨¡å‹ (ä¸­åº¦ä½¿ç”¨)
â”‚   â”œâ”€â”€ HuggingFace Transformers
â”‚   â””â”€â”€ æ¨¡å‹è®­ç»ƒå’Œä¼˜åŒ–
â”œâ”€â”€ âŒ Level 3: RAGç³»ç»Ÿ (ä¸éœ€è¦)
â”œâ”€â”€ âŒ Level 4: Agentå¼€å‘ (ä¸éœ€è¦)
â””â”€â”€ ğŸŸ¡ Level 5: å®æˆ˜é¡¹ç›® (å¯é€‰ï¼Œçœ‹é¡¹ç›®ç±»å‹)
```

### å¿«é€Ÿè‡ªæµ‹

**å¦‚æœä½ èƒ½çœ‹æ‡‚å¹¶å†™å‡ºä¸‹é¢çš„ä»£ç ï¼Œå¯ä»¥è·³è¿‡Week 0ï¼Œç›´æ¥ä»Week 1å¼€å§‹:**

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

# 1. Tensoræ“ä½œ
x = torch.randn(2, 3, 4)  # batch_size=2, seq_len=3, d_model=4
y = x.transpose(1, 2)     # è½¬ç½®
z = torch.matmul(x, y)    # çŸ©é˜µä¹˜æ³•

# 2. å®šä¹‰ç®€å•æ¨¡å‹
class SimpleAttention(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        self.query = nn.Linear(d_model, d_model)
        self.key = nn.Linear(d_model, d_model)
        
    def forward(self, x):
        q = self.query(x)
        k = self.key(x)
        scores = torch.matmul(q, k.transpose(-2, -1))
        attention = F.softmax(scores, dim=-1)
        return attention

# 3. è®­ç»ƒå¾ªç¯
model = SimpleAttention(d_model=64)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

for epoch in range(10):
    optimizer.zero_grad()
    output = model(x)
    loss = output.mean()  # ç®€åŒ–çš„loss
    loss.backward()
    optimizer.step()
```

**å¦‚æœä¸Šé¢çš„ä»£ç ä½ çœ‹ä¸æ‡‚æˆ–å†™ä¸å‡ºæ¥ï¼Œè¯·ä»Week 0å¼€å§‹å­¦ä¹ ã€‚**

### å­¦ä¹ è·¯å¾„é€‰æ‹©

#### è·¯å¾„A: å®Œæ•´å­¦ä¹  (æ¨è - é€‚åˆæƒ³æ·±å…¥ç†è§£AIçš„äºº)
```
Week 0: PyTorchåŸºç¡€
  â†“
Week 1-2: Transformerå®ç°
  â†“
Week 3-4: LLMæ·±å…¥
  â†“
Week 5+: RAG & Agentå¼€å‘
```
**ä¼˜åŠ¿**: 
- çœŸæ­£ç†è§£AIå·¥ä½œåŸç†
- èƒ½æ‰‹æ’¸Transformer (é¢è¯•åŠ åˆ†)
- å¯ä»¥è‡ªå®šä¹‰æ¨¡å‹å’Œè®­ç»ƒ
- å›½å†…å¤§å‚AIå²—å¿…å¤‡

#### è·¯å¾„B: å¿«é€Ÿåº”ç”¨ (é€‚åˆèµ¶æ—¶é—´æˆ–åªåšåº”ç”¨å±‚çš„äºº)
```
ç›´æ¥è·³åˆ°Week 5: RAGç³»ç»Ÿ
  â†“
Week 6+: Agentå¼€å‘
  â†“
æœ‰éœ€è¦æ—¶å†å›æ¥å­¦PyTorch
```
**ä¼˜åŠ¿**: 
- å¿«é€Ÿä¸Šæ‰‹AIåº”ç”¨å¼€å‘
- å…ˆåšäº§å“ï¼Œåå­¦åŸç†
- é€‚åˆå…¨æ ˆå·¥ç¨‹å¸ˆå¿«é€Ÿè½¬å‹

### æˆ‘çš„å»ºè®®

**åŸºäºä½ çš„èƒŒæ™¯ (CS + å…¨æ ˆ + é‡åŒ–äº¤æ˜“)**ï¼Œå¼ºçƒˆæ¨è **è·¯å¾„A: å®Œæ•´å­¦ä¹ **

åŸå› :
1. ä½ æœ‰CSåŸºç¡€ï¼Œå­¦PyTorchå¾ˆå¿« (1å‘¨è¶³å¤Ÿ)
2. é‡åŒ–äº¤æ˜“ä¸­MLæ¨¡å‹å¾ˆå¸¸ç”¨ï¼ŒPyTorchæ˜¯å¿…å¤‡æŠ€èƒ½
3. å›½å†…å¤§å‚é¢è¯•å¿…è€ƒ"æ‰‹æ’¸Transformer"
4. ç†è§£åº•å±‚åŸç†è®©ä½ åœ¨AIåº”ç”¨å¼€å‘ä¸­æ›´æœ‰ä¼˜åŠ¿
5. ä½ çš„ç»ˆæé¡¹ç›® (é‡åŒ–äº¤æ˜“Agent) å¯èƒ½éœ€è¦è‡ªå®šä¹‰MLæ¨¡å‹

---

## ğŸ¯ å­¦ä¹ è·¯çº¿å›¾

```
Week 0: PyTorchåŸºç¡€é€Ÿæˆ (1å‘¨) - æ–°å¢!
    â†“
Level 1: TransformeråŸºç¡€ (2-3å‘¨) - éœ€è¦PyTorch
    â†“
Level 2: LLMå·¥ä½œåŸç† (1-2å‘¨) - éœ€è¦PyTorch
    â†“
Level 3: Prompt Engineering & RAG (1å‘¨) - ä¸éœ€è¦PyTorch
    â†“
Level 4: AI Agentæ¶æ„ (2-3å‘¨) - ä¸éœ€è¦PyTorch
    â†“
Level 5: å®æˆ˜é¡¹ç›® (æŒç»­) - å¯é€‰ä½¿ç”¨PyTorch
```

**æ€»æ—¶é•¿**: 13å‘¨ (åŒ…å«PyTorch Week 0)

**æ ¸å¿ƒç†å¿µ**: 
- å…ˆæ‰“å¥½PyTorchåŸºç¡€ (Week 0)
- å†æ·±å…¥TransformeråŸç† (Week 1-4)
- æœ€åæ„å»ºAIåº”ç”¨ (Week 5-13)

---

## ğŸ”¥ Week 0: PyTorchåŸºç¡€é€Ÿæˆ

**å¦‚æœä½ å·²ç»ä¼šPyTorchï¼Œè·³è¿‡è¿™å‘¨ç›´æ¥åˆ°Week 1**

### å­¦ä¹ ç›®æ ‡
- ç†è§£Tensoræ“ä½œå’Œè‡ªåŠ¨å¾®åˆ†
- èƒ½ç”¨nn.Moduleå®šä¹‰ç¥ç»ç½‘ç»œ
- æŒæ¡åŸºæœ¬çš„è®­ç»ƒå¾ªç¯
- ä¸ºTransformerå®ç°åšå‡†å¤‡

### Day 1-2: PyTorchæ ¸å¿ƒæ¦‚å¿µ

#### èµ„æº1: PyTorchå®˜æ–¹60åˆ†é’Ÿæ•™ç¨‹ (ğŸ”¥ æœ€é‡è¦)
- **é“¾æ¥**: https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html
- **æ—¶é•¿**: 2-3å°æ—¶
- **å­¦ä¹ é‡ç‚¹**:
  - TensoråŸºç¡€æ“ä½œ
  - Autogradè‡ªåŠ¨å¾®åˆ†
  - ç¥ç»ç½‘ç»œnn.Module
  - è®­ç»ƒç¥ç»ç½‘ç»œ

**å®è·µä»»åŠ¡**:
```python
# Task 1: Tensoræ“ä½œç»ƒä¹ 
import torch

# åˆ›å»ºtensor
x = torch.randn(3, 4)
y = torch.ones(4, 5)

# çŸ©é˜µä¹˜æ³•
z = torch.matmul(x, y)
print(z.shape)  # torch.Size([3, 5])

# ç»´åº¦æ“ä½œ (Transformerä¸­è¶…å¸¸ç”¨!)
a = torch.randn(2, 3, 4)  # [batch, seq_len, d_model]
b = a.transpose(1, 2)     # [batch, d_model, seq_len]
c = a.view(2, -1)         # [batch, seq_len * d_model]

# Task 2: ç†è§£Autograd
x = torch.tensor([2.0], requires_grad=True)
y = x ** 2 + 3 * x
y.backward()
print(x.grad)  # dy/dx = 2*x + 3 = 7.0
```

#### èµ„æº2: Andrej Karpathy - micrograd
- **é“¾æ¥**: https://github.com/karpathy/micrograd
- **è§†é¢‘**: https://www.youtube.com/watch?v=VMj-3S1tku0
- **æ—¶é•¿**: 2å°æ—¶
- **ä¸ºä»€ä¹ˆé‡è¦**: ä»é›¶å®ç°autogradï¼Œç†è§£åå‘ä¼ æ’­æœ¬è´¨
- **å­¦ä¹ é‡ç‚¹**:
  - è®¡ç®—å›¾çš„æ„å»º
  - åå‘ä¼ æ’­ç®—æ³•
  - æ¢¯åº¦çš„é“¾å¼æ³•åˆ™

**å®è·µä»»åŠ¡**:
```python
# Clone microgradå¹¶è¿è¡Œ
git clone https://github.com/karpathy/micrograd.git
cd micrograd
python demo.py

# ç†è§£Valueç±»å¦‚ä½•å®ç°è‡ªåŠ¨å¾®åˆ†
# å°è¯•æ·»åŠ æ–°çš„æ“ä½œ (æ¯”å¦‚ exp, log)
```

### Day 3-4: ç¥ç»ç½‘ç»œåŸºç¡€

#### èµ„æº3: PyTorch for Deep Learning (Zero to Mastery)
- **é“¾æ¥**: https://www.learnpytorch.io/
- **ç« èŠ‚**: 00-02ç« 
- **ä¸ºä»€ä¹ˆæ¨è**: éå¸¸é€‚åˆæœ‰ç¼–ç¨‹åŸºç¡€çš„äººï¼Œä»£ç ä¸ºä¸»

**å®è·µä»»åŠ¡ 1: çº¿æ€§å›å½’**
```python
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

# ç”Ÿæˆæ•°æ®
X = torch.linspace(0, 10, 100).reshape(-1, 1)
y = 2 * X + 1 + torch.randn(100, 1) * 0.5

# å®šä¹‰æ¨¡å‹
class LinearRegression(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(1, 1)
    
    def forward(self, x):
        return self.linear(x)

# è®­ç»ƒ
model = LinearRegression()
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

losses = []
for epoch in range(100):
    # Forward pass
    predictions = model(X)
    loss = criterion(predictions, y)
    
    # Backward pass
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    losses.append(loss.item())
    if epoch % 10 == 0:
        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')

# å¯è§†åŒ–
plt.plot(losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.show()

# æµ‹è¯•
with torch.no_grad():
    test_x = torch.tensor([[5.0]])
    prediction = model(test_x)
    print(f'Prediction for x=5: {prediction.item():.2f}')
```

**å®è·µä»»åŠ¡ 2: MNISTæ‰‹å†™æ•°å­—åˆ†ç±»**
```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# 1. æ•°æ®å‡†å¤‡
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST('./data', train=False, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)

# 2. å®šä¹‰æ¨¡å‹
class SimpleNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(28 * 28, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 10)
        self.dropout = nn.Dropout(0.2)
    
    def forward(self, x):
        x = x.view(-1, 28 * 28)  # Flatten
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# 3. è®­ç»ƒå‡½æ•°
def train(model, device, train_loader, optimizer, epoch):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = nn.CrossEntropyLoss()(output, target)
        loss.backward()
        optimizer.step()
        
        if batch_idx % 100 == 0:
            print(f'Epoch {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}] Loss: {loss.item():.4f}')

# 4. æµ‹è¯•å‡½æ•°
def test(model, device, test_loader):
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += nn.CrossEntropyLoss()(output, target).item()
            pred = output.argmax(dim=1)
            correct += pred.eq(target).sum().item()
    
    test_loss /= len(test_loader)
    accuracy = 100. * correct / len(test_loader.dataset)
    print(f'\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\n')

# 5. è¿è¡Œè®­ç»ƒ
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = SimpleNN().to(device)
optimizer = optim.Adam(model.parameters(), lr=0.001)

for epoch in range(1, 6):
    train(model, device, train_loader, optimizer, epoch)
    test(model, device, test_loader)

# ä¿å­˜æ¨¡å‹
torch.save(model.state_dict(), 'mnist_model.pth')
```

### Day 5-6: Transformerå‡†å¤‡ - é‡è¦çš„ç»´åº¦æ“ä½œ

**ä¸ºä»€ä¹ˆé‡è¦**: Transformerä¸­æœ€éš¾çš„å°±æ˜¯å¤„ç† [batch, seq_len, d_model] è¿™æ ·çš„3D tensor

#### èµ„æº4: Understanding Tensor Dimensions
- **é“¾æ¥**: https://pytorch.org/tutorials/beginner/nn_tutorial.html
- **é‡ç‚¹**: Broadcasting, view, transpose, reshape

**å®è·µä»»åŠ¡ 3: æŒæ¡Transformerä¸­çš„tensoræ“ä½œ**
```python
import torch
import torch.nn as nn

# Transformerä¸­çš„å…¸å‹ç»´åº¦
batch_size = 2
seq_len = 5
d_model = 8
num_heads = 4

# 1. æ¨¡æ‹Ÿä¸€ä¸ªbatchçš„è¾“å…¥åºåˆ—
x = torch.randn(batch_size, seq_len, d_model)
print(f"Input shape: {x.shape}")  # [2, 5, 8]

# 2. Multi-head attentionéœ€è¦æ‹†åˆ†head
d_k = d_model // num_heads  # 8 // 4 = 2
# Reshape: [batch, seq_len, d_model] -> [batch, seq_len, num_heads, d_k]
x_split = x.view(batch_size, seq_len, num_heads, d_k)
# Transpose: [batch, seq_len, num_heads, d_k] -> [batch, num_heads, seq_len, d_k]
x_heads = x_split.transpose(1, 2)
print(f"Multi-head shape: {x_heads.shape}")  # [2, 4, 5, 2]

# 3. Attentionçš„çŸ©é˜µä¹˜æ³•
Q = x_heads  # [batch, num_heads, seq_len, d_k]
K = x_heads  # [batch, num_heads, seq_len, d_k]
V = x_heads  # [batch, num_heads, seq_len, d_k]

# Q @ K^T: [batch, num_heads, seq_len, d_k] @ [batch, num_heads, d_k, seq_len]
#        = [batch, num_heads, seq_len, seq_len]
scores = torch.matmul(Q, K.transpose(-2, -1))
print(f"Attention scores shape: {scores.shape}")  # [2, 4, 5, 5]

# 4. Softmax
attention_weights = torch.softmax(scores, dim=-1)
print(f"Attention weights shape: {attention_weights.shape}")  # [2, 4, 5, 5]

# 5. Attention @ V
output = torch.matmul(attention_weights, V)
print(f"Attention output shape: {output.shape}")  # [2, 4, 5, 2]

# 6. åˆå¹¶heads
# [batch, num_heads, seq_len, d_k] -> [batch, seq_len, num_heads, d_k]
output = output.transpose(1, 2)
# [batch, seq_len, num_heads, d_k] -> [batch, seq_len, d_model]
output = output.contiguous().view(batch_size, seq_len, d_model)
print(f"Final output shape: {output.shape}")  # [2, 5, 8]
```

**ç»ƒä¹ **: å®ç°ä¸€ä¸ªç®€åŒ–çš„scaled dot-product attention
```python
def scaled_dot_product_attention(Q, K, V, mask=None):
    """
    Args:
        Q: [batch, num_heads, seq_len, d_k]
        K: [batch, num_heads, seq_len, d_k]
        V: [batch, num_heads, seq_len, d_k]
        mask: [batch, 1, 1, seq_len] or None
    
    Returns:
        output: [batch, num_heads, seq_len, d_k]
        attention_weights: [batch, num_heads, seq_len, seq_len]
    """
    d_k = Q.size(-1)
    
    # 1. è®¡ç®—attention scores
    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))
    
    # 2. åº”ç”¨mask (å¯é€‰)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, float('-inf'))
    
    # 3. Softmax
    attention_weights = torch.softmax(scores, dim=-1)
    
    # 4. åŠ æƒæ±‚å’Œ
    output = torch.matmul(attention_weights, V)
    
    return output, attention_weights

# æµ‹è¯•
Q = torch.randn(2, 4, 5, 2)
K = torch.randn(2, 4, 5, 2)
V = torch.randn(2, 4, 5, 2)

output, weights = scaled_dot_product_attention(Q, K, V)
print(f"Output shape: {output.shape}")  # [2, 4, 5, 2]
print(f"Attention weights shape: {weights.shape}")  # [2, 4, 5, 5]
```

### Day 7: PyTorchè¿›é˜¶æŠ€å·§

#### èµ„æº5: PyTorch Performance Tips
- **é“¾æ¥**: https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html
- **å­¦ä¹ é‡ç‚¹**:
  - GPUä½¿ç”¨ (.to(device), .cuda())
  - Batch processing
  - DataLoaderä½¿ç”¨
  - æ¨¡å‹ä¿å­˜å’ŒåŠ è½½

**å®è·µä»»åŠ¡ 4: GPUåŠ é€Ÿ**
```python
import torch
import time

# æ£€æŸ¥GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# CPU vs GPUé€Ÿåº¦å¯¹æ¯”
size = 5000

# CPU
x_cpu = torch.randn(size, size)
y_cpu = torch.randn(size, size)

start = time.time()
z_cpu = torch.matmul(x_cpu, y_cpu)
cpu_time = time.time() - start
print(f"CPU time: {cpu_time:.4f}s")

# GPU (if available)
if torch.cuda.is_available():
    x_gpu = x_cpu.to(device)
    y_gpu = y_cpu.to(device)
    
    # Warm up
    _ = torch.matmul(x_gpu, y_gpu)
    torch.cuda.synchronize()
    
    start = time.time()
    z_gpu = torch.matmul(x_gpu, y_gpu)
    torch.cuda.synchronize()
    gpu_time = time.time() - start
    print(f"GPU time: {gpu_time:.4f}s")
    print(f"Speedup: {cpu_time / gpu_time:.2f}x")
```

**å®è·µä»»åŠ¡ 5: æ¨¡å‹ä¿å­˜å’ŒåŠ è½½**
```python
# ä¿å­˜æ•´ä¸ªæ¨¡å‹
torch.save(model, 'entire_model.pth')

# ä¿å­˜æ¨¡å‹å‚æ•° (æ¨è)
torch.save(model.state_dict(), 'model_weights.pth')

# åŠ è½½æ¨¡å‹
model = SimpleNN()
model.load_state_dict(torch.load('model_weights.pth'))
model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

# ä¿å­˜è®­ç»ƒcheckpoint (åŒ…å«optimizerçŠ¶æ€)
checkpoint = {
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': loss,
}
torch.save(checkpoint, 'checkpoint.pth')

# åŠ è½½checkpointç»§ç»­è®­ç»ƒ
checkpoint = torch.load('checkpoint.pth')
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
epoch = checkpoint['epoch']
loss = checkpoint['loss']
```

### Week 0æ€»ç»“æ£€éªŒ

**ä½ åº”è¯¥èƒ½å¤Ÿ**:

- [ ] åˆ›å»ºå’Œæ“ä½œtensorï¼Œç†è§£shape, view, transpose
- [ ] è§£é‡Šautogradå’Œbackward()çš„å·¥ä½œåŸç†
- [ ] ç”¨nn.Moduleå®šä¹‰è‡ªå·±çš„ç¥ç»ç½‘ç»œ
- [ ] ç¼–å†™å®Œæ•´çš„è®­ç»ƒå¾ªç¯ (forward, loss, backward, step)
- [ ] å¤„ç†[batch, seq_len, d_model]è¿™æ ·çš„3D tensor
- [ ] å®ç°ç®€å•çš„scaled dot-product attention
- [ ] ä½¿ç”¨GPUåŠ é€Ÿ
- [ ] ä¿å­˜å’ŒåŠ è½½æ¨¡å‹

**éªŒæ”¶é¡¹ç›®**: 
å®ŒæˆMNISTåˆ†ç±»ï¼Œæµ‹è¯•å‡†ç¡®ç‡è¾¾åˆ°95%ä»¥ä¸Šï¼Œå¹¶èƒ½è§£é‡Šæ¯ä¸€è¡Œä»£ç çš„ä½œç”¨ã€‚

**å¦‚æœå®Œæˆä»¥ä¸Šä»»åŠ¡ï¼Œä½ å·²ç»å‡†å¤‡å¥½å¼€å§‹Week 1çš„Transformerå®ç°äº†ï¼**

### é¢å¤–å­¦ä¹ èµ„æº (å¯é€‰)

#### æ·±å…¥ç†è§£PyTorch
- **Dive into Deep Learning (D2L) PyTorchç‰ˆ**
  - é“¾æ¥: https://d2l.ai/
  - ç« èŠ‚: Chapter 2-3
  - é€‚åˆ: æƒ³æ›´ç³»ç»Ÿå­¦ä¹ çš„äºº

#### PyTorchå†…éƒ¨æœºåˆ¶
- **PyTorch Internals**
  - é“¾æ¥: http://blog.ezyang.com/2019/05/pytorch-internals/
  - é€‚åˆ: æƒ³äº†è§£PyTorchå¦‚ä½•å·¥ä½œçš„äºº

#### è§†é¢‘æ•™ç¨‹
- **PyTorch Tutorials by sentdex**
  - é“¾æ¥: https://www.youtube.com/playlist?list=PLQVvvaa0QuDdeMyHEYc0gxFpYwHY2Qfdh
  - é€‚åˆ: å–œæ¬¢çœ‹è§†é¢‘å­¦ä¹ çš„äºº

---

## ğŸ“š Level 1: TransformeråŸºç¡€

**å­¦ä¹ ç›®æ ‡**: ç†è§£Transformeræ¶æ„ï¼Œèƒ½æ‰‹å†™æ ¸å¿ƒç»„ä»¶

### å¿…çœ‹è§†é¢‘

#### 1. Andrej Karpathy - Let's build GPT (ğŸ”¥ æœ€é‡è¦)
- **é“¾æ¥**: https://www.youtube.com/watch?v=kCc8FmEb1nY
- **æ—¶é•¿**: 2å°æ—¶
- **ä¸ºä»€ä¹ˆé‡è¦**: ä»é›¶å®ç°GPTï¼Œè®²è§£æœ€æ¸…æ™°
- **å­¦ä¹ é‡ç‚¹**: 
  - Self-attentionæœºåˆ¶
  - Positional encoding
  - Multi-head attention
  - Layer normalization

#### 2. 3Blue1Brown - Attentionæœºåˆ¶å¯è§†åŒ–
- **é“¾æ¥**: https://www.youtube.com/watch?v=eMlx5fFNoYc
- **æ—¶é•¿**: 30åˆ†é’Ÿ
- **ä¸ºä»€ä¹ˆé‡è¦**: è§†è§‰åŒ–ç†è§£attention
- **å­¦ä¹ é‡ç‚¹**: 
  - Query, Key, Valueçš„å«ä¹‰
  - Attentionæƒé‡è®¡ç®—
  - ä¸ºä»€ä¹ˆå«"attention"

#### 3. StatQuest - Transformerè¯¦è§£
- **é“¾æ¥**: https://www.youtube.com/watch?v=zxQyTK8quyY
- **æ—¶é•¿**: 45åˆ†é’Ÿ
- **ä¸ºä»€ä¹ˆé‡è¦**: æ•°å­¦åŸç†è®²å¾—å¾ˆå¥½
- **å­¦ä¹ é‡ç‚¹**: 
  - Scaled dot-product attention
  - Softmaxçš„ä½œç”¨
  - æ®‹å·®è¿æ¥

### å¿…è¯»æ•™ç¨‹

#### 4. The Illustrated Transformer (ğŸ”¥ æ–°æ‰‹å¿…è¯»)
- **é“¾æ¥**: http://jalammar.github.io/illustrated-transformer/
- **ä¸ºä»€ä¹ˆé‡è¦**: å›¾è§£ç‰ˆï¼Œç†è§£æœ€ç›´è§‚
- **å­¦ä¹ é‡ç‚¹**: 
  - Encoder-Decoderæ¶æ„
  - æ¯ä¸€å±‚çš„è¾“å…¥è¾“å‡º
  - Transformerå…¨æµç¨‹

#### 5. Annotated Transformer (Harvard NLP)
- **é“¾æ¥**: https://nlp.seas.harvard.edu/annotated-transformer/
- **ä¸ºä»€ä¹ˆé‡è¦**: å¸¦æ³¨é‡Šçš„å®Œæ•´ä»£ç å®ç°
- **å­¦ä¹ é‡ç‚¹**: 
  - PyTorchå®ç°ç»†èŠ‚
  - è®­ç»ƒå¾ªç¯
  - Batchå¤„ç†

#### 6. åŸå§‹è®ºæ–‡ (å¯é€‰)
- **é“¾æ¥**: https://arxiv.org/abs/1706.03762
- **æ ‡é¢˜**: "Attention is All You Need"
- **å»ºè®®**: å…ˆçœ‹ä¸Šé¢çš„æ•™ç¨‹ï¼Œå†å›æ¥çœ‹è®ºæ–‡

### å®è·µé¡¹ç›®

#### 7. nanoGPT (ğŸ”¥ æœ€é‡è¦çš„å®è·µ)
- **é“¾æ¥**: https://github.com/karpathy/nanoGPT
- **ä¸ºä»€ä¹ˆé‡è¦**: æœ€ç®€åŒ–çš„GPTå®ç°
- **ä»»åŠ¡**: 
  - [ ] Cloneä»“åº“å¹¶è¿è¡Œ
  - [ ] ç†è§£æ¯ä¸€è¡Œä»£ç 
  - [ ] åœ¨å°æ•°æ®é›†ä¸Šè®­ç»ƒ
  - [ ] ä¿®æ”¹æ¨¡å‹å‚æ•°è§‚å¯Ÿæ•ˆæœ

#### 8. minGPT
- **é“¾æ¥**: https://github.com/karpathy/minGPT
- **ä¸ºä»€ä¹ˆé‡è¦**: æ•™å­¦ç‰ˆæœ¬ï¼Œæ³¨é‡Šè¯¦ç»†
- **ä»»åŠ¡**: 
  - [ ] å¯¹æ¯”nanoGPTå’ŒminGPTçš„åŒºåˆ«
  - [ ] æ‰‹å†™attention layer
  - [ ] å®ç°è‡ªå·±çš„mini-transformer

### æ£€éªŒæ ‡å‡†
- [ ] èƒ½ç”¨PyTorchä»é›¶å®ç°scaled dot-product attention
- [ ] ç†è§£multi-head attentionçš„ä½œç”¨
- [ ] èƒ½è§£é‡Špositional encodingä¸ºä»€ä¹ˆå¿…è¦
- [ ] èƒ½ç”»å‡ºTransformerçš„æ¶æ„å›¾

---

## ğŸ“š Level 2: LLMå·¥ä½œåŸç†

**å­¦ä¹ ç›®æ ‡**: ç†è§£å¤§æ¨¡å‹å¦‚ä½•è®­ç»ƒã€æ¨ç†ã€å¯¹é½

### ç†è§£å¤§æ¨¡å‹è®­ç»ƒ

#### 9. Stanford CS324 - LLMè¯¾ç¨‹
- **é“¾æ¥**: https://stanford-cs324.github.io/winter2022/
- **ä¸ºä»€ä¹ˆé‡è¦**: å®Œæ•´çš„LLMç†è®ºè¯¾ç¨‹
- **å­¦ä¹ é‡ç‚¹**: 
  - Pre-training vs Fine-tuning
  - Model scaling laws
  - Emergent abilities
  - Inference optimization

#### 10. Andrej Karpathy - State of GPT
- **é“¾æ¥**: https://www.youtube.com/watch?v=bZQun8Y4L2A
- **æ—¶é•¿**: 1å°æ—¶
- **ä¸ºä»€ä¹ˆé‡è¦**: GPTçš„è®­ç»ƒå…¨æµç¨‹
- **å­¦ä¹ é‡ç‚¹**: 
  - Pre-trainingé˜¶æ®µ
  - Supervised fine-tuning
  - RLHFè¿‡ç¨‹
  - æ•°æ®è´¨é‡çš„é‡è¦æ€§

#### 11. LLMå¯è§†åŒ– (ğŸ”¥ å¿…ç©)
- **é“¾æ¥**: https://bbycroft.net/llm
- **ä¸ºä»€ä¹ˆé‡è¦**: äº¤äº’å¼çœ‹GPTå¦‚ä½•ç”Ÿæˆæ–‡å­—
- **ä»»åŠ¡**: 
  - [ ] è¾“å…¥ä¸åŒpromptè§‚å¯Ÿtokenç”Ÿæˆ
  - [ ] ç†è§£temperatureå‚æ•°çš„å½±å“
  - [ ] çœ‹attention pattern

### å…³é”®æ¦‚å¿µ

#### 12. Understanding RLHF
- **é“¾æ¥**: https://huggingface.co/blog/rlhf
- **ä¸ºä»€ä¹ˆé‡è¦**: ç†è§£ChatGPTå¦‚ä½•å¯¹é½äººç±»åå¥½
- **å­¦ä¹ é‡ç‚¹**: 
  - Reward modelè®­ç»ƒ
  - PPOç®—æ³•
  - ä¸ºä»€ä¹ˆéœ€è¦RLHF

#### 13. Tokenizationè¯¦è§£
- **é“¾æ¥**: https://www.youtube.com/watch?v=zduSFxRajkE
- **ä¸ºä»€ä¹ˆé‡è¦**: Karpathyè®²tokenizer
- **å­¦ä¹ é‡ç‚¹**: 
  - BPEç®—æ³•
  - Token vs Character
  - Tokenizationå¯¹æ¨¡å‹çš„å½±å“

### æ£€éªŒæ ‡å‡†
- [ ] ç†è§£pre-trainingå’Œfine-tuningçš„åŒºåˆ«
- [ ] èƒ½è§£é‡ŠRLHFçš„å·¥ä½œåŸç†
- [ ] ç†è§£temperatureã€top-pç­‰é‡‡æ ·å‚æ•°
- [ ] çŸ¥é“tokenizationå¦‚ä½•å½±å“æ¨¡å‹æ€§èƒ½

---

## ğŸ“š Level 3: Prompt Engineering & RAG

**å­¦ä¹ ç›®æ ‡**: æŒæ¡é«˜æ•ˆä½¿ç”¨LLMçš„æ–¹æ³•ï¼Œå®ç°RAGç³»ç»Ÿ

### Prompt Engineering

#### 14. OpenAI Prompt Engineering Guide
- **é“¾æ¥**: https://platform.openai.com/docs/guides/prompt-engineering
- **ä¸ºä»€ä¹ˆé‡è¦**: å®˜æ–¹æœ€ä½³å®è·µ
- **å­¦ä¹ é‡ç‚¹**: 
  - Few-shot learning
  - Chain-of-thought prompting
  - System messagesè®¾è®¡
  - å¦‚ä½•å‡å°‘hallucination

#### 15. Anthropic Prompt Engineering
- **é“¾æ¥**: https://docs.anthropic.com/claude/docs/prompt-engineering
- **ä¸ºä»€ä¹ˆé‡è¦**: Claudeçš„promptingæŠ€å·§
- **å­¦ä¹ é‡ç‚¹**: 
  - XML tagsä½¿ç”¨
  - Long contextå¤„ç†
  - Role prompting
  - Citation patterns

#### 16. Learn Prompting (å…è´¹è¯¾ç¨‹)
- **é“¾æ¥**: https://learnprompting.org/
- **ä¸ºä»€ä¹ˆé‡è¦**: ç³»ç»ŸåŒ–å­¦ä¹ 
- **ä»»åŠ¡**: 
  - [ ] å®ŒæˆåŸºç¡€è¯¾ç¨‹
  - [ ] ç»ƒä¹ å„ç§promptingæŠ€å·§
  - [ ] å¯¹æ¯”ä¸åŒæ–¹æ³•çš„æ•ˆæœ

### RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ)

#### 17. LangChain RAG Tutorial
- **é“¾æ¥**: https://python.langchain.com/docs/tutorials/rag/
- **ä¸ºä»€ä¹ˆé‡è¦**: å®ç°ä½ è‡ªå·±çš„RAGç³»ç»Ÿ
- **å­¦ä¹ é‡ç‚¹**: 
  - Document loading
  - Text splittingç­–ç•¥
  - Embeddingé€‰æ‹©
  - Retrieval methods

#### 18. Pinecone Learning Center
- **é“¾æ¥**: https://www.pinecone.io/learn/retrieval-augmented-generation/
- **ä¸ºä»€ä¹ˆé‡è¦**: RAGç†è®º+å®è·µ
- **å­¦ä¹ é‡ç‚¹**: 
  - Vector databaseåŸç†
  - Semantic search
  - Hybrid search
  - Re-ranking strategies

#### 19. RAGè®ºæ–‡è§£è¯»
- **é“¾æ¥**: https://arxiv.org/abs/2005.11401
- **æ ‡é¢˜**: "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"
- **å»ºè®®**: ç†è§£å®è·µåå†çœ‹è®ºæ–‡

### å®è·µé¡¹ç›®
**ä»»åŠ¡**: æ„å»ºä¸€ä¸ª"ä½ çš„è‚¡ç¥¨ç ”ç©¶ç¬”è®°"RAG chatbot
- [ ] æ”¶é›†ä½ çš„é‡åŒ–äº¤æ˜“ç¬”è®°ä½œä¸ºçŸ¥è¯†åº“
- [ ] å®ç°æ–‡æ¡£embeddingå’Œå­˜å‚¨
- [ ] æ„å»ºæ£€ç´¢+ç”Ÿæˆpipeline
- [ ] æµ‹è¯•ä¸åŒretrievalç­–ç•¥çš„æ•ˆæœ

### æ£€éªŒæ ‡å‡†
- [ ] ç†è§£few-shot vs zero-shot prompting
- [ ] èƒ½è®¾è®¡æœ‰æ•ˆçš„system prompt
- [ ] ç†è§£RAGçš„å·¥ä½œæµç¨‹
- [ ] èƒ½å®ç°ä¸€ä¸ªå®Œæ•´çš„RAGç³»ç»Ÿ

---

## ğŸ“š Level 4: AI Agentæ¶æ„

**å­¦ä¹ ç›®æ ‡**: ç†è§£Agentè®¾è®¡æ¨¡å¼ï¼Œæ„å»ºè‡ªä¸»å†³ç­–ç³»ç»Ÿ

### Agentè®¾è®¡æ¨¡å¼

#### 20. DeepLearning.AI - Agentic Design Patterns (ä½ å·²ç»åœ¨çœ‹çš„)
- **é“¾æ¥**: https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/
- **ä¸ºä»€ä¹ˆé‡è¦**: Andrew Ngçš„è¯¾ç¨‹
- **å­¦ä¹ é‡ç‚¹**: 
  - Reflection pattern
  - Tool use pattern
  - Planning pattern
  - Multi-agent collaboration

#### 21. LangChain Agentsæ–‡æ¡£
- **é“¾æ¥**: https://python.langchain.com/docs/concepts/agents/
- **ä¸ºä»€ä¹ˆé‡è¦**: Agentå®ç°æ¡†æ¶
- **å­¦ä¹ é‡ç‚¹**: 
  - Agent types (ReAct, Function calling)
  - Tool integration
  - Agent executor
  - Streaming & callbacks

#### 22. ReActè®ºæ–‡ (ğŸ”¥ æ ¸å¿ƒè®ºæ–‡)
- **é“¾æ¥**: https://arxiv.org/abs/2210.03629
- **æ ‡é¢˜**: "ReAct: Synergizing Reasoning and Acting in Language Models"
- **ä¸ºä»€ä¹ˆé‡è¦**: Reasoning + ActingèŒƒå¼
- **å­¦ä¹ é‡ç‚¹**: 
  - Thought-Action-Observationå¾ªç¯
  - ä¸ºä»€ä¹ˆéœ€è¦reasoning
  - å¦‚ä½•è®¾è®¡action space

### Agentæ¡†æ¶å®æˆ˜

#### 23. AutoGPTæºç 
- **é“¾æ¥**: https://github.com/Significant-Gravitas/AutoGPT
- **ä¸ºä»€ä¹ˆé‡è¦**: ç ”ç©¶çœŸå®çš„agentå®ç°
- **ä»»åŠ¡**: 
  - [ ] Cloneå¹¶è¿è¡ŒAutoGPT
  - [ ] ç†è§£å…¶agent loop
  - [ ] åˆ†ætoolè°ƒç”¨æœºåˆ¶
  - [ ] çœ‹å®ƒå¦‚ä½•ç®¡ç†memory

#### 24. LangGraph (ğŸ”¥ æœ€å¥½çš„Agentå·¥å…·)
- **é“¾æ¥**: https://langchain-ai.github.io/langgraph/
- **ä¸ºä»€ä¹ˆé‡è¦**: çŠ¶æ€æœºå¼çš„agentæ¡†æ¶
- **å­¦ä¹ é‡ç‚¹**: 
  - Graph-based agent design
  - State management
  - Conditional edges
  - Human-in-the-loop

#### 25. Anthropic Computer Use
- **é“¾æ¥**: https://docs.anthropic.com/en/docs/build-with-claude/computer-use
- **ä¸ºä»€ä¹ˆé‡è¦**: Claudeæ§åˆ¶ç”µè„‘çš„agentå®ç°
- **å­¦ä¹ é‡ç‚¹**: 
  - Vision + Actionç»“åˆ
  - Tool callingå®ç°
  - Error handling
  - Safety considerations

### Agentæ ¸å¿ƒæ¦‚å¿µ

#### 26. Tool Callingè¯¦è§£
- **é“¾æ¥**: https://platform.openai.com/docs/guides/function-calling
- **ä¸ºä»€ä¹ˆé‡è¦**: Agentå¦‚ä½•è°ƒç”¨å·¥å…·
- **å­¦ä¹ é‡ç‚¹**: 
  - Function schemaè®¾è®¡
  - Tool selectionç­–ç•¥
  - Error handling
  - Parallel tool calling

#### 27. Memory Management
- **é“¾æ¥**: https://python.langchain.com/docs/how_to/#memory
- **ä¸ºä»€ä¹ˆé‡è¦**: Agentå¦‚ä½•è®°å¿†å¯¹è¯
- **å­¦ä¹ é‡ç‚¹**: 
  - Short-term vs long-term memory
  - Conversation buffer
  - Summary memory
  - Vector store memory

#### 28. Multi-Agentç³»ç»Ÿ
- **é“¾æ¥**: https://microsoft.github.io/autogen/
- **ä¸ºä»€ä¹ˆé‡è¦**: å¾®è½¯çš„å¤šagentæ¡†æ¶
- **å­¦ä¹ é‡ç‚¹**: 
  - Agent communication protocols
  - Task delegation
  - Consensus mechanisms
  - Multi-agent orchestration

### æ£€éªŒæ ‡å‡†
- [ ] ç†è§£ReAct agentçš„å·¥ä½œæµç¨‹
- [ ] èƒ½ç”¨LangGraphæ„å»ºæœ‰çŠ¶æ€çš„agent
- [ ] ç†è§£tool callingçš„å®ç°åŸç†
- [ ] èƒ½è®¾è®¡multi-agentç³»ç»Ÿæ¶æ„

---

## ğŸ“š Level 5: å®æˆ˜é¡¹ç›®

**å­¦ä¹ ç›®æ ‡**: æ„å»ºç«¯åˆ°ç«¯çš„AIåº”ç”¨

### ä»ç®€å•åˆ°å¤æ‚

#### 29. æ„å»ºä¸€ä¸ªRAG chatbot
- **é“¾æ¥**: https://github.com/langchain-ai/rag-from-scratch
- **ä¸ºä»€ä¹ˆé‡è¦**: å®Œæ•´çš„RAGé¡¹ç›®
- **ä»»åŠ¡**: 
  - [ ] å®ç°document ingestion pipeline
  - [ ] æ„å»ºwebç•Œé¢ (ç”¨ä½ çš„ReactæŠ€èƒ½)
  - [ ] æ·»åŠ conversation memory
  - [ ] éƒ¨ç½²åˆ°production

#### 30. Build a Research Assistant Agent
- **é“¾æ¥**: https://www.deeplearning.ai/short-courses/ai-agents-in-langgraph/
- **ä¸ºä»€ä¹ˆé‡è¦**: DeepLearning.AIè¯¾ç¨‹
- **ä»»åŠ¡**: 
  - [ ] å®Œæˆè¯¾ç¨‹é¡¹ç›®
  - [ ] æ‰©å±•ä¸ºå¤šåŠŸèƒ½research agent
  - [ ] æ·»åŠ web search capability
  - [ ] å®ç°citation tracking

#### 31. Multi-Agent Trading System (ğŸ”¥ ç»“åˆé‡åŒ–äº¤æ˜“!)
- **é“¾æ¥**: https://github.com/langchain-ai/langchain/blob/master/cookbook/multi_agent_trading_system.ipynb
- **ä¸ºä»€ä¹ˆé‡è¦**: ç”¨agentsåšè‚¡ç¥¨åˆ†æ
- **é¡¹ç›®è§„åˆ’**: è§ä¸‹æ–¹"ç»ˆæé¡¹ç›®"éƒ¨åˆ†

### ç»ˆæé¡¹ç›®: é‡åŒ–äº¤æ˜“Agentç³»ç»Ÿ

**é¡¹ç›®ç»“æ„**:
```
Trading Agent System
â”œâ”€â”€ Research Agent
â”‚   â”œâ”€â”€ æœç´¢è´¢æŠ¥
â”‚   â”œâ”€â”€ çˆ¬å–æ–°é—»
â”‚   â””â”€â”€ ç¤¾äº¤åª’ä½“æƒ…ç»ªåˆ†æ
â”œâ”€â”€ Analysis Agent
â”‚   â”œâ”€â”€ æŠ€æœ¯åˆ†æ (ä½ å­¦çš„TAçŸ¥è¯†)
â”‚   â”œâ”€â”€ åŸºæœ¬é¢åˆ†æ
â”‚   â””â”€â”€ é‡åŒ–æŒ‡æ ‡è®¡ç®—
â”œâ”€â”€ Strategy Agent
â”‚   â”œâ”€â”€ ç”Ÿæˆäº¤æ˜“ç­–ç•¥
â”‚   â”œâ”€â”€ Backtesting
â”‚   â””â”€â”€ å‚æ•°ä¼˜åŒ–
â””â”€â”€ Risk Management Agent
    â”œâ”€â”€ ä»“ä½ç®¡ç†
    â”œâ”€â”€ æ­¢æŸç­–ç•¥
    â””â”€â”€ é£é™©è¯„ä¼°
```

**æŠ€æœ¯æ ˆ**:
- Frontend: React + TypeScript (ä½ å·²æœ‰çš„æŠ€èƒ½)
- Backend: Python + FastAPI
- Agents: LangGraph
- Database: PostgreSQL + Pinecone
- Data: yfinance, pandas, numpy

**é¡¹ç›®é˜¶æ®µ**:
1. **Week 1-2**: å•ä¸€Research Agent
2. **Week 3-4**: æ·»åŠ Analysis Agent
3. **Week 5-6**: æ„å»ºStrategy Agent
4. **Week 7-8**: é›†æˆRisk Management
5. **Week 9-10**: Webç•Œé¢å¼€å‘
6. **Week 11-12**: ä¼˜åŒ–å’Œéƒ¨ç½²

**é¢„æœŸæˆæœ**:
- ä¸€ä¸ªå¯ä»¥è‡ªåŠ¨ç ”ç©¶è‚¡ç¥¨çš„AIç³»ç»Ÿ
- ç»“åˆä½ çš„é‡åŒ–äº¤æ˜“çŸ¥è¯†
- å®Œæ•´çš„portfolioé¡¹ç›®
- é¢è¯•æ—¶çš„å¼ºå¤§äº®ç‚¹

---

## ğŸ—“ï¸ 13å‘¨è¯¦ç»†å­¦ä¹ è®¡åˆ’

### Week 0: PyTorchåŸºç¡€é€Ÿæˆ (æ–°å¢)

**æœ¬å‘¨ç›®æ ‡**: æŒæ¡PyTorchåŸºç¡€ï¼Œä¸ºTransformerå®ç°åšå‡†å¤‡

**å­¦ä¹ ä»»åŠ¡**:
- [ ] Day 1-2: PyTorch 60åˆ†é’Ÿæ•™ç¨‹ + micrograd
- [ ] Day 3-4: çº¿æ€§å›å½’ + MNISTåˆ†ç±»é¡¹ç›®
- [ ] Day 5-6: æŒæ¡Transformerä¸­çš„tensorç»´åº¦æ“ä½œ
- [ ] Day 7: GPUä½¿ç”¨å’Œæ¨¡å‹ä¿å­˜/åŠ è½½

**å®è·µé¡¹ç›®**:
- å®ŒæˆMNISTæ‰‹å†™æ•°å­—åˆ†ç±» (å‡†ç¡®ç‡>95%)
- å®ç°ç®€åŒ–ç‰ˆçš„scaled dot-product attention
- èƒ½å¤Ÿç†Ÿç»ƒå¤„ç† [batch, seq_len, d_model] ç»´åº¦

**æ£€éªŒæ ‡å‡†**:
```python
# ä½ éœ€è¦èƒ½è½»æ¾å†™å‡ºè¿™æ ·çš„ä»£ç 
class SimpleAttention(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
    
    def forward(self, x):
        # x: [batch, seq_len, d_model]
        Q = self.W_q(x)
        K = self.W_k(x)
        V = self.W_v(x)
        
        d_k = Q.size(-1)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
        attention = F.softmax(scores, dim=-1)
        output = torch.matmul(attention, V)
        
        return output
```

---

### Week 1-2: TransformeråŸºç¡€

**æœ¬å‘¨ç›®æ ‡**: ç†è§£å¹¶èƒ½æ‰‹å†™Transformeræ ¸å¿ƒç»„ä»¶

**å­¦ä¹ ä»»åŠ¡**:
- [ ] Day 1-2: çœ‹Karpathyçš„GPTè§†é¢‘ (2å°æ—¶)
- [ ] Day 3-4: è¯»The Illustrated Transformerï¼Œåšç¬”è®°
- [ ] Day 5-6: Clone nanoGPTï¼Œé€è¡Œç†è§£ä»£ç 
- [ ] Day 7-8: æ‰‹å†™attention layer
- [ ] Day 9-10: åœ¨toy datasetä¸Šè®­ç»ƒmini-GPT
- [ ] Day 11-14: å®ŒæˆAnnotated Transformeræ•™ç¨‹

**å®è·µé¡¹ç›®**:
```python
# ä½ éœ€è¦èƒ½å†™å‡ºè¿™æ ·çš„ä»£ç 
class ScaledDotProductAttention(nn.Module):
    def forward(self, Q, K, V, mask=None):
        d_k = Q.size(-1)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))
        attention = F.softmax(scores, dim=-1)
        return torch.matmul(attention, V), attention

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        assert d_model % num_heads == 0
        self.d_k = d_model // num_heads
        self.num_heads = num_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
    
    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        
        # Linear projections
        Q = self.W_q(query)  # [batch, seq_len, d_model]
        K = self.W_k(key)
        V = self.W_v(value)
        
        # Split into multiple heads
        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        # Now: [batch, num_heads, seq_len, d_k]
        
        # Scaled dot-product attention
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))
        attention = F.softmax(scores, dim=-1)
        output = torch.matmul(attention, V)
        
        # Concatenate heads
        output = output.transpose(1, 2).contiguous()
        output = output.view(batch_size, -1, self.num_heads * self.d_k)
        
        # Final linear
        return self.W_o(output)
```

**æ£€éªŒæ ‡å‡†**:
- èƒ½åœ¨ç™½æ¿ä¸Šç”»å‡ºTransformeræ¶æ„
- èƒ½è§£é‡Šattentionæƒé‡çš„è®¡ç®—
- èƒ½è¿è¡Œå¹¶ä¿®æ”¹nanoGPT
- èƒ½æ‰‹å†™multi-head attention

---

### Week 3-4: æ·±å…¥LLM

**æœ¬å‘¨ç›®æ ‡**: ç†è§£å¤§æ¨¡å‹è®­ç»ƒå’Œæ¨ç†å…¨æµç¨‹

**å­¦ä¹ ä»»åŠ¡**:
- [ ] Day 1-3: Stanford CS324å‰5è®²
- [ ] Day 4-5: çœ‹State of GPTè§†é¢‘
- [ ] Day 6-7: ç†è§£tokenization (çœ‹Karpathyè§†é¢‘)
- [ ] Day 8-9: ç©LLMå¯è§†åŒ–å·¥å…·ï¼Œè§‚å¯Ÿtokenç”Ÿæˆ
- [ ] Day 10-11: å­¦ä¹ RLHFåŸç†
- [ ] Day 12-14: ç”¨HuggingFace fine-tuneä¸€ä¸ªå°æ¨¡å‹

**å®è·µé¡¹ç›®**:
- Fine-tuneä¸€ä¸ªGPT-2 smallåœ¨ä½ çš„domainæ•°æ®ä¸Š
- å¯¹æ¯”ä¸åŒè®­ç»ƒç­–ç•¥çš„æ•ˆæœ
- å®éªŒä¸åŒçš„é‡‡æ ·å‚æ•°

**æ£€éªŒæ ‡å‡†**:
- ç†è§£pre-training vs fine-tuningåŒºåˆ«
- èƒ½è§£é‡ŠRLHFå¦‚ä½•å·¥ä½œ
- çŸ¥é“temperatureå¦‚ä½•å½±å“ç”Ÿæˆ

---

### Week 5: RAGç³»ç»Ÿ

**æœ¬å‘¨ç›®æ ‡**: æ„å»ºä¸€ä¸ªå®Œæ•´çš„RAGåº”ç”¨

**å­¦ä¹ ä»»åŠ¡**:
- [ ] Day 1-2: LangChain RAG tutorial
- [ ] Day 3-4: å­¦ä¹ vector database (Pinecone/Chroma)
- [ ] Day 5-7: å®è·µé¡¹ç›® (è§ä¸‹æ–¹)

**å®è·µé¡¹ç›®**: **è‚¡ç¥¨ç ”ç©¶ç¬”è®°RAG Chatbot**
```
åŠŸèƒ½:
1. ä¸Šä¼ ä½ çš„é‡åŒ–äº¤æ˜“ç¬”è®° (PDF/Markdown)
2. è‡ªåŠ¨chunkingå’Œembedding
3. è‡ªç„¶è¯­è¨€æŸ¥è¯¢ä½ çš„ç¬”è®°
4. æ˜¾ç¤ºcitationå’Œæ¥æº

æŠ€æœ¯:
- Document loader: LangChain
- Embedding: OpenAI embeddings
- Vector store: Chroma (æœ¬åœ°å…è´¹)
- LLM: GPT-3.5 æˆ– Claude
- Frontend: ç®€å•çš„Streamlitç•Œé¢
```

**æ£€éªŒæ ‡å‡†**:
- RAGç³»ç»Ÿèƒ½æ­£ç¡®æ£€ç´¢ç›¸å…³æ–‡æ¡£
- å›ç­”å‡†ç¡®ä¸”æœ‰citation
- ç†è§£ä¸åŒchunkingç­–ç•¥çš„å½±å“

---

### Week 6-7: AgentåŸºç¡€

**æœ¬å‘¨ç›®æ ‡**: ç†è§£agentè®¾è®¡æ¨¡å¼ï¼Œå®ç°ReAct agent

**å­¦ä¹ ä»»åŠ¡**:
- [ ] Day 1-3: å®ŒæˆDeepLearning.AI agentic patternsè¯¾ç¨‹
- [ ] Day 4-5: è¯»ReActè®ºæ–‡ï¼Œç†è§£reasoningè¿‡ç¨‹
- [ ] Day 6-8: å­¦ä¹ tool callingæœºåˆ¶
- [ ] Day 9-10: LangChain agentsæ–‡æ¡£
- [ ] Day 11-14: å®è·µé¡¹ç›® (è§ä¸‹æ–¹)

**å®è·µé¡¹ç›®**: **ç®€å•çš„ReAct Agent**
```python
# å®ç°ä¸€ä¸ªèƒ½åšæ•°å­¦è®¡ç®—çš„agent
tools = [
    Calculator(),      # åŸºç¡€è®¡ç®—
    WebSearch(),       # æœç´¢ä¿¡æ¯
    PythonREPL()       # æ‰§è¡ŒPythonä»£ç 
]

# Agentèƒ½å›ç­”:
# "2023å¹´ç‰¹æ–¯æ‹‰è‚¡ä»·æ¶¨å¹…æ˜¯å¤šå°‘?"
# 1. æœç´¢ç‰¹æ–¯æ‹‰2023è‚¡ä»·æ•°æ®
# 2. ç”¨è®¡ç®—å™¨ç®—æ¶¨å¹…
# 3. è¿”å›ç­”æ¡ˆ
```

**æ£€éªŒæ ‡å‡†**:
- Agentèƒ½æ­£ç¡®é€‰æ‹©å’Œä½¿ç”¨å·¥å…·
- ç†è§£thought-action-observationå¾ªç¯
- èƒ½å¤„ç†multi-step reasoning

---

### Week 8-9: Agentæ¡†æ¶æ·±å…¥

**æœ¬å‘¨ç›®æ ‡**: æŒæ¡LangGraphï¼Œæ„å»ºæœ‰çŠ¶æ€çš„agent

**å­¦ä¹ ä»»åŠ¡**:
- [ ] Day 1-4: LangGraph tutorials
- [ ] Day 5-7: ç ”ç©¶AutoGPTæºç 
- [ ] Day 8-10: å­¦ä¹ multi-agenté€šä¿¡
- [ ] Day 11-14: å®è·µé¡¹ç›® (è§ä¸‹æ–¹)

**å®è·µé¡¹ç›®**: **æœ‰çŠ¶æ€çš„å¯¹è¯Agent**
```
åŠŸèƒ½:
1. è®°ä½å¯¹è¯å†å²
2. å¤šè½®è§„åˆ’å’Œæ‰§è¡Œ
3. å¤„ç†ç”¨æˆ·åé¦ˆ
4. é”™è¯¯é‡è¯•æœºåˆ¶

ç¤ºä¾‹åœºæ™¯:
User: "å¸®æˆ‘åˆ†æä¸€ä¸‹NVDAçš„æŠ•èµ„ä»·å€¼"
Agent: 
- State 1: æœç´¢NVDAåŸºæœ¬ä¿¡æ¯
- State 2: è·å–è´¢åŠ¡æ•°æ®
- State 3: è¿›è¡ŒæŠ€æœ¯åˆ†æ
- State 4: ç”Ÿæˆç»¼åˆæŠ¥å‘Š
- (æ¯ä¸ªstateå¯ä»¥æ ¹æ®ç»“æœè°ƒæ•´)
```

**æ£€éªŒæ ‡å‡†**:
- èƒ½ç”¨LangGraphæ„å»ºcomplex workflow
- ç†è§£state management
- èƒ½å®ç°human-in-the-loop

---

### Week 10-12: ç»¼åˆé¡¹ç›®

**ç»ˆæé¡¹ç›®**: **Multi-Agenté‡åŒ–äº¤æ˜“ç³»ç»Ÿ**

**Phase 1 (Week 10): Research Agent**
```python
class ResearchAgent:
    """è´Ÿè´£æ”¶é›†å’Œæ•´ç†ä¿¡æ¯"""
    tools = [
        SECFilingsTool(),      # è´¢æŠ¥æ•°æ®
        NewsTool(),            # æ–°é—»æœç´¢
        SocialSentimentTool(), # Reddit/Twitteræƒ…ç»ª
    ]
    
    def research_stock(self, ticker: str):
        # æ”¶é›†æ‰€æœ‰ç›¸å…³ä¿¡æ¯
        pass
```

**Phase 2 (Week 11): Analysis & Strategy Agents**
```python
class AnalysisAgent:
    """æŠ€æœ¯åˆ†æå’ŒåŸºæœ¬é¢åˆ†æ"""
    def analyze(self, stock_data, research_data):
        technical = self.technical_analysis(stock_data)
        fundamental = self.fundamental_analysis(research_data)
        return combined_analysis

class StrategyAgent:
    """ç”Ÿæˆäº¤æ˜“ç­–ç•¥"""
    def generate_strategy(self, analysis):
        # åŸºäºåˆ†æç”Ÿæˆå…·ä½“ç­–ç•¥
        pass
```

**Phase 3 (Week 12): é›†æˆå’ŒUI**
```typescript
// Reactå‰ç«¯ (ç”¨ä½ çš„æŠ€èƒ½!)
const TradingDashboard = () => {
  return (
    <div>
      <StockSearchBar />
      <AgentStatus />  {/* æ˜¾ç¤ºå„agentçŠ¶æ€ */}
      <ResearchPanel /> {/* Research Agentè¾“å‡º */}
      <AnalysisPanel /> {/* Analysis Agentè¾“å‡º */}
      <StrategyPanel /> {/* Strategyå»ºè®® */}
      <RiskMetrics />  {/* é£é™©æŒ‡æ ‡ */}
    </div>
  );
};
```

**æœ€ç»ˆäº¤ä»˜**:
- [ ] å®Œæ•´çš„multi-agentç³»ç»Ÿ
- [ ] Webç•Œé¢
- [ ] æ–‡æ¡£å’Œdemoè§†é¢‘
- [ ] GitHub repo (ä½œä¸ºportfolio)

---

## ğŸ”— é¢å¤–èµ„æº

### ä¿æŒæ›´æ–°

#### 32. Papers with Code - Transformers
- **é“¾æ¥**: https://paperswithcode.com/methods/category/transformers
- **ç”¨é€”**: æœ€æ–°ç ”ç©¶è¿›å±•
- **å»ºè®®**: æ¯å‘¨æµè§ˆä¸€æ¬¡

#### 33. Hugging Face Course
- **é“¾æ¥**: https://huggingface.co/learn/nlp-course/
- **ç”¨é€”**: NLPå’ŒTransformerså®Œæ•´è¯¾ç¨‹
- **å»ºè®®**: ä½œä¸ºè¡¥å……å­¦ä¹ ææ–™

#### 34. AI Agentè®ºå›
- **é“¾æ¥**: https://www.reddit.com/r/LangChain/
- **ç”¨é€”**: ç¤¾åŒºè®¨è®ºå’Œé—®é¢˜è§£ç­”
- **å»ºè®®**: é‡åˆ°é—®é¢˜æ—¶æŸ¥æ‰¾æˆ–æé—®

### æ¨èä¹¦ç±

1. **"Deep Learning" by Goodfellow et al.**
   - æ·±åº¦å­¦ä¹ åœ£ç»
   - https://www.deeplearningbook.org/

2. **"Speech and Language Processing" by Jurafsky**
   - NLPåŸºç¡€
   - https://web.stanford.edu/~jurafsky/slp3/

3. **"Designing Data-Intensive Applications"**
   - æ„å»ºproduction AIç³»ç»Ÿå¿…è¯»
   - ç†è§£scalabilityå’Œreliability

4. **"Programming PyTorch for Deep Learning" by Ian Pointer** (æ–°å¢)
   - PyTorchå®æˆ˜æŒ‡å—
   - é€‚åˆå¿«é€Ÿä¸Šæ‰‹

### YouTubeé¢‘é“

1. **Andrej Karpathy**
   - https://www.youtube.com/@AndrejKarpathy
   - ä»é›¶æ„å»ºGPTç³»åˆ—

2. **StatQuest with Josh Starmer**
   - https://www.youtube.com/@statquest
   - æœºå™¨å­¦ä¹ æ¦‚å¿µå¯è§†åŒ–

3. **Two Minute Papers**
   - https://www.youtube.com/@TwoMinutePapers
   - å¿«é€Ÿäº†è§£æœ€æ–°AIç ”ç©¶

4. **sentdex** (æ–°å¢)
   - https://www.youtube.com/@sentdex
   - PyTorchå’Œæ·±åº¦å­¦ä¹ æ•™ç¨‹

### PyTorchä¸“é¢˜èµ„æº (æ–°å¢)

#### å®˜æ–¹èµ„æº
- **PyTorch Documentation**: https://pytorch.org/docs/stable/index.html
- **PyTorch Tutorials**: https://pytorch.org/tutorials/
- **PyTorch Examples**: https://github.com/pytorch/examples

#### ç¤¾åŒºèµ„æº
- **PyTorch Forums**: https://discuss.pytorch.org/
- **PyTorch Lightning Docs**: https://lightning.ai/docs/pytorch/stable/
- **Hugging Face Course**: https://huggingface.co/learn/nlp-course/chapter0/1

#### é«˜çº§ä¸»é¢˜
- **Mixed Precision Training**: https://pytorch.org/docs/stable/amp.html
- **Distributed Training**: https://pytorch.org/tutorials/beginner/dist_overview.html
- **Model Optimization**: https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html

### å­¦ä¹ ç¤¾åŒº

1. **Reddit**
   - r/MachineLearning: https://www.reddit.com/r/MachineLearning/
   - r/PyTorch: https://www.reddit.com/r/PyTorch/
   - r/LangChain: https://www.reddit.com/r/LangChain/

2. **Discord Servers**
   - PyTorch Discord: https://discord.gg/pytorch
   - Hugging Face Discord: https://discord.gg/JfAtkvEtRb

3. **Twitter/X å…³æ³¨**
   - @karpathy (Andrej Karpathy)
   - @PyTorch (PyTorchå®˜æ–¹)
   - @huggingface (Hugging Face)
   - @AnthropicAI (Anthropic/Claude)

---

## ğŸ’¡ å­¦ä¹ å»ºè®®

### åŸåˆ™

1. **ä¸è¦è·³æ­¥**: æ¯ä¸ªleveléƒ½è¦æ‰å®æŒæ¡å†å‰è¿›
   - Week 0 (PyTorch) æ˜¯åŸºç¡€ï¼Œä¸€å®šè¦æ‰å®
   - å¦‚æœWeek 0çš„æ£€éªŒé¡¹ç›®åšä¸å‡ºæ¥ï¼Œä¸è¦è¿›å…¥Week 1

2. **ä»£ç ä¸ºä¸»**: 80%æ—¶é—´å†™ä»£ç ï¼Œ20%çœ‹ç†è®º
   - çœ‹è§†é¢‘æ—¶ä¸€å®šè¦è·Ÿç€æ•²ä»£ç 
   - ä¸è¦åªæ”¶è—èµ„æºä¸å®è·µ

3. **å°æ­¥å¿«è·‘**: æ¯å‘¨ä¸€ä¸ªå¯è¿è¡Œçš„å°é¡¹ç›®
   - Week 0: MNISTåˆ†ç±»å™¨
   - Week 1-2: Mini transformer
   - Week 3-4: Fine-tunedæ¨¡å‹
   - Week 5: RAG chatbot
   - æ¯å‘¨éƒ½è¦æœ‰èƒ½demoçš„ä¸œè¥¿

4. **ç»“åˆå…´è¶£**: é‡åŒ–äº¤æ˜“æ˜¯ä½ çš„ä¼˜åŠ¿ï¼Œå……åˆ†åˆ©ç”¨
   - ç”¨è‚¡ç¥¨æ•°æ®åšè®­ç»ƒ
   - RAGç³»ç»Ÿç”¨ä½ çš„äº¤æ˜“ç¬”è®°
   - æœ€ç»ˆé¡¹ç›®æ˜¯é‡åŒ–äº¤æ˜“agent

### æ—¶é—´åˆ†é…

**æ¯å¤©3-4å°æ—¶** (å¯è°ƒæ•´):
- 1å°æ—¶: çœ‹è§†é¢‘/è¯»æ–‡ç« 
- 2å°æ—¶: å†™ä»£ç /åšé¡¹ç›®
- 0.5å°æ—¶: ç¬”è®°å’Œæ€»ç»“
- 0.5å°æ—¶: å’Œæˆ‘è®¨è®ºé—®é¢˜

**æ¯å‘¨æœ«**:
- 2å°æ—¶: å¤ä¹ æœ¬å‘¨å†…å®¹
- 2å°æ—¶: å®Œæˆå‘¨é¡¹ç›®
- 1å°æ—¶: è§„åˆ’ä¸‹å‘¨å­¦ä¹ 
- å†™ä¸€ç¯‡æ€»ç»“blog (å¯é€‰ä½†æ¨è)

**å¦‚æœæ—¶é—´æœ‰é™** (æ¯”å¦‚æ¯å¤©åªæœ‰2å°æ—¶):
- å»¶é•¿è®¡åˆ’åˆ°20å‘¨
- æˆ–è€…è·³è¿‡Week 0ï¼Œä»Level 3å¼€å§‹ (RAGå’ŒAgent)
- å…ˆåšåº”ç”¨ï¼Œåå­¦åŸç†

### å­¦ä¹ æŠ€å·§

1. **è´¹æ›¼å­¦ä¹ æ³•**: 
   - æ¯å‘¨å†™ä¸€ç¯‡blogè§£é‡Šå­¦åˆ°çš„æ¦‚å¿µ
   - å‡è£…ä½ åœ¨æ•™åˆ«äºº
   - å¦‚æœä½ è§£é‡Šä¸æ¸…æ¥šï¼Œè¯´æ˜è¿˜æ²¡çœŸæ­£ç†è§£
   - æ¨èå¹³å°: Medium, Dev.to, æˆ–ä¸ªäººGitHub Pages

2. **Project-based Learning**:
   - ä¸è¦åªçœ‹tutorial
   - æ¯ä¸ªæ¦‚å¿µéƒ½è¦æœ‰å¯¹åº”çš„ä»£ç å®è·µ
   - æ”¹è¿›æ•™ç¨‹ä¸­çš„ä»£ç ï¼Œæ·»åŠ è‡ªå·±çš„feature
   - çŠ¯é”™æ˜¯å­¦ä¹ æœ€å¿«çš„æ–¹å¼

3. **è®°å½•è¿‡ç¨‹**:
   - **GitHub**: è®°å½•æ‰€æœ‰ä»£ç 
     - åˆ›å»ºä¸€ä¸ª "learning-ai" repo
     - æ¯å‘¨ä¸€ä¸ªæ–‡ä»¶å¤¹: week-0-pytorch, week-1-transformerç­‰
     - å†™å¥½READMEè¯´æ˜æ¯ä¸ªé¡¹ç›®
   - **Notion/Obsidian**: è®°å½•ç¬”è®°
     - æ¦‚å¿µè§£é‡Š
     - é‡åˆ°çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ
     - èµ„æºé“¾æ¥æ•´ç†
   - **ä¸ºé¢è¯•åšå‡†å¤‡**: è¿™äº›éƒ½æ˜¯ä½ çš„portfolio

4. **ä¸»åŠ¨å­¦ä¹ ** (ğŸ”¥ é‡è¦):
   ```
   è¢«åŠ¨å­¦ä¹  (æ•ˆç‡ä½):
   çœ‹è§†é¢‘ â†’ ç‚¹ç‚¹å¤´ â†’ å…³æ‰ â†’ å¿˜è®°
   
   ä¸»åŠ¨å­¦ä¹  (æ•ˆç‡é«˜):
   çœ‹è§†é¢‘ â†’ æš‚åœ â†’ è‡ªå·±å®ç° â†’ é‡åˆ°bug â†’ è°ƒè¯• â†’ ç†è§£
   ```

5. **é—´éš”é‡å¤**:
   - å­¦å®Œä¸€ä¸ªæ¦‚å¿µå
   - ç¬¬2å¤©: å›é¡¾
   - ç¬¬7å¤©: å¤ä¹ 
   - ç¬¬30å¤©: å†æ¬¡å¤ä¹ 
   - ç”¨Ankiæˆ–Notionåˆ¶ä½œflashcards

### é¿å…çš„å‘

âŒ **åªçœ‹ä¸ç»ƒ** - æœ€å¤§çš„å‘
- æ”¶è—äº†100ä¸ªæ•™ç¨‹ä½†ä¸€ä¸ªéƒ½æ²¡åšå®Œ
- è§£å†³æ–¹æ³•: ç«‹åˆ»å®è·µï¼Œçœ‹ä¸€ä¸ªåšä¸€ä¸ª

âŒ **è¿½æ±‚å®Œç¾ä¸»ä¹‰**
- åœ¨Week 0å¡ä¸€ä¸ªæœˆæƒ³æŠŠPyTorchå­¦é€
- è§£å†³æ–¹æ³•: å¤Ÿç”¨å°±è¡Œï¼Œè¾¹ç”¨è¾¹å­¦

âŒ **è·³ç€å­¦ï¼ŒåŸºç¡€ä¸æ‰å®**
- PyTorchä¸ä¼šå°±å»å­¦Transformer
- è§£å†³æ–¹æ³•: ä¸¥æ ¼æŒ‰Week 0 â†’ Week 1é¡ºåº

âŒ **ä¸åšç¬”è®°ï¼Œå­¦äº†å°±å¿˜**
- 3ä¸ªæœˆåå®Œå…¨æƒ³ä¸èµ·æ¥å­¦è¿‡ä»€ä¹ˆ
- è§£å†³æ–¹æ³•: æ¯å¤©å†™å­¦ä¹ æ—¥å¿—

âŒ **å­¤å†›å¥‹æˆ˜**
- é‡åˆ°é—®é¢˜ä¸é—®ï¼Œè‡ªå·±æ­»ç£•å‡ å¤©
- è§£å†³æ–¹æ³•: åŠæ—¶é—®æˆ‘ï¼Œæˆ–ä¸Šè®ºå›/Discord

âŒ **åªå­¦ä¸ç”¨**
- å­¦äº†ä¸€å †ç†è®ºï¼Œä¸çŸ¥é“æ€ä¹ˆåº”ç”¨
- è§£å†³æ–¹æ³•: ä»Week 1å¼€å§‹å°±æ€è€ƒå®é™…åº”ç”¨åœºæ™¯

âœ… **æ­£ç¡®åšæ³•**:
- å¿«é€Ÿè¿­ä»£ï¼Œè¾¹å­¦è¾¹åš
- æ¯å‘¨ä¸€ä¸ªå¯demoçš„é¡¹ç›®
- ä¸»åŠ¨åˆ†äº«å’Œè®¨è®º
- é‡åˆ°å›°éš¾åŠæ—¶æ±‚åŠ©
- è®°å½•å­¦ä¹ è¿‡ç¨‹
- ç»“åˆå®é™…éœ€æ±‚å­¦ä¹ 

### è°ƒè¯•æŠ€å·§ (PyTorchç‰¹å®š)

```python
# 1. æ£€æŸ¥tensor shape (æœ€å¸¸è§çš„bug)
print(f"x.shape: {x.shape}")
assert x.shape == (batch_size, seq_len, d_model), f"Expected shape {(batch_size, seq_len, d_model)}, got {x.shape}"

# 2. æ£€æŸ¥æ¢¯åº¦
for name, param in model.named_parameters():
    if param.grad is not None:
        print(f"{name}: grad norm = {param.grad.norm()}")
    else:
        print(f"{name}: NO GRADIENT!")

# 3. æ£€æŸ¥æ˜¯å¦æœ‰NaN
assert not torch.isnan(loss).any(), "Loss contains NaN!"

# 4. å¯è§†åŒ–attention weights
import matplotlib.pyplot as plt
import seaborn as sns

attention = attention_weights[0, 0].detach().cpu().numpy()  # [seq_len, seq_len]
sns.heatmap(attention, cmap='viridis')
plt.show()

# 5. ä½¿ç”¨PyTorchçš„debuggingå·¥å…·
torch.autograd.set_detect_anomaly(True)  # æ£€æµ‹backwardä¸­çš„é—®é¢˜
```

### æ±‚åŠ©æ¸ é“

**é‡åˆ°é—®é¢˜æ—¶**:

1. **å…ˆGoogle**: "pytorch [your error message]"
2. **æŸ¥å®˜æ–¹æ–‡æ¡£**: https://pytorch.org/docs/
3. **æœç´¢Stack Overflow**: 90%çš„é—®é¢˜å·²ç»æœ‰ç­”æ¡ˆ
4. **é—®æˆ‘**: éšæ—¶è”ç³»
5. **è®ºå›**: PyTorch Discuss, Reddit
6. **GitHub Issues**: å¦‚æœæ˜¯åº“çš„bug

**æé—®çš„æ­£ç¡®æ–¹å¼**:
```
âŒ ä¸å¥½çš„æé—®:
"æˆ‘çš„ä»£ç ä¸å·¥ä½œï¼Œæ€ä¹ˆåŠ?"

âœ… å¥½çš„æé—®:
"æˆ‘åœ¨å®ç°multi-head attentionæ—¶é‡åˆ°shapeä¸åŒ¹é…çš„é”™è¯¯:
RuntimeError: mat1 and mat2 shapes cannot be multiplied (2x5 and 8x8)

æˆ‘çš„ä»£ç :
[é™„ä¸Šminimal reproducible example]

æˆ‘çš„ç†è§£æ˜¯...
æˆ‘å°è¯•äº†...
ä½†æ˜¯...

è¯·é—®æ˜¯å“ªé‡Œå‡ºäº†é—®é¢˜?"
```

---

## ğŸ“Š æŠ€èƒ½æ ‘è¿›é˜¶è·¯å¾„

```
ç°æœ‰æŠ€èƒ½                 â†’  AIæŠ€èƒ½
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
React/TypeScript        â†’  æ„å»ºAgent UI
Python (é‡åŒ–äº¤æ˜“)       â†’  å®ç°Agenté€»è¾‘
å…¨æ ˆå¼€å‘ç»éªŒ            â†’  End-to-end Agentç³»ç»Ÿ
APIé›†æˆ                â†’  Tool callingè®¾è®¡
æ•°æ®åˆ†æ                â†’  Model evaluation
ç³»ç»Ÿè®¾è®¡æ€ç»´            â†’  Multi-agentæ¶æ„
```

### ç‹¬ç‰¹ä¼˜åŠ¿

ä½ çš„èƒŒæ™¯ç»„åˆéå¸¸ç¨€ç¼º:
- âœ… å·¥ç¨‹èƒ½åŠ›å¼º (å…¨æ ˆå¼€å‘)
- âœ… æœ‰domain knowledge (é‡åŒ–äº¤æ˜“)
- âœ… æ•°å­¦åŸºç¡€å¥½ (CSèƒŒæ™¯)
- âœ… å®è·µç»éªŒä¸°å¯Œ (å®ä¹ é¡¹ç›®)

è¿™è®©ä½ åœ¨AIåº”ç”¨å¼€å‘ä¸Šæœ‰å·¨å¤§ä¼˜åŠ¿ï¼Œå°¤å…¶æ˜¯:
- **FinTeché¢†åŸŸ**: AI + é‡åŒ–äº¤æ˜“
- **AIå·¥å…·å¼€å‘**: æ‡‚ç”¨æˆ·éœ€æ±‚çš„AI engineer
- **åˆ›ä¸šæ–¹å‘**: AI-powered trading tools

---

## ğŸ¯ èŒä¸šå‘å±•è·¯å¾„

### çŸ­æœŸ (3-6ä¸ªæœˆ)

**ç›®æ ‡**: æŒæ¡AI Agentå¼€å‘
- å®Œæˆæœ¬å­¦ä¹ è®¡åˆ’
- æ„å»º2-3ä¸ªportfolioé¡¹ç›®
- åœ¨GitHubç§¯ç´¯ä»£ç 

**é¢è¯•å‡†å¤‡**:
- TransformeråŸç† (æ‰‹å†™ä»£ç )
- RAGç³»ç»Ÿè®¾è®¡
- Agentæ¶æ„è®¨è®º
- å®é™…é¡¹ç›®ç»éªŒ

### ä¸­æœŸ (6-12ä¸ªæœˆ)

**ç›®æ ‡**: æˆä¸ºAIåº”ç”¨ä¸“å®¶
- æ·±å…¥æŸä¸ªå‚ç›´é¢†åŸŸ (æ¨èFinTech)
- è´¡çŒ®å¼€æºé¡¹ç›® (LangChain, LangGraphç­‰)
- å†™æŠ€æœ¯åšå®¢
- å‚åŠ AI hackathons

**æ½œåœ¨å…¬å¸**:
- é‡åŒ–ç§å‹Ÿ (Two Sigma, Citadel)
- FinTech (Stripe, Plaid, Robinhood)
- AI Infra (Anthropic, OpenAI, Scale AI)
- ä¼ ç»Ÿç§‘æŠ€å¤§å‚çš„AI team

### é•¿æœŸ (1-2å¹´+)

**å¯èƒ½æ–¹å‘**:

1. **AI Research Engineer**
   - æ”¹è¿›model architecture
   - ä¼˜åŒ–training/inference
   - å‘è®ºæ–‡

2. **AI Product Engineer**
   - æ„å»ºAI-powered products
   - ç”¨æˆ·ä½“éªŒä¼˜åŒ–
   - Product-market fit

3. **åˆ›ä¸š**
   - AI trading tools
   - Developer tools for AI
   - Vertical AI agents

---

## ğŸ“ æ£€æŸ¥æ¸…å•

### Level 1 å®Œæˆæ ‡å‡†
- [ ] èƒ½æ‰‹å†™scaled dot-product attention
- [ ] ç†è§£multi-head attentionåŸç†
- [ ] è§£é‡Špositional encodingä½œç”¨
- [ ] è®­ç»ƒè¿‡è‡³å°‘ä¸€ä¸ªtoy transformer
- [ ] èƒ½ç”»å‡ºå®Œæ•´çš„transformeræ¶æ„å›¾

### Level 2 å®Œæˆæ ‡å‡†
- [ ] ç†è§£pre-training vs fine-tuning
- [ ] è§£é‡ŠRLHFå·¥ä½œæµç¨‹
- [ ] çŸ¥é“tokenizationå¦‚ä½•å½±å“æ€§èƒ½
- [ ] Fine-tunedè¿‡è‡³å°‘ä¸€ä¸ªæ¨¡å‹
- [ ] ç†è§£inference optimizationæŠ€æœ¯

### Level 3 å®Œæˆæ ‡å‡†
- [ ] æŒæ¡few-shot prompting
- [ ] èƒ½è®¾è®¡æœ‰æ•ˆçš„system prompt
- [ ] å®ç°è¿‡å®Œæ•´çš„RAGç³»ç»Ÿ
- [ ] ç†è§£vector databaseåŸç†
- [ ] å¯¹æ¯”è¿‡ä¸åŒretrievalç­–ç•¥

### Level 4 å®Œæˆæ ‡å‡†
- [ ] ç†è§£ReAct agentå·¥ä½œæµ
- [ ] ç”¨LangGraphæ„å»ºè¿‡agent
- [ ] å®ç°è¿‡tool calling
- [ ] ç†è§£multi-agenté€šä¿¡
- [ ] è®¾è®¡è¿‡agentæ¶æ„

### Level 5 å®Œæˆæ ‡å‡†
- [ ] å®Œæˆé‡åŒ–äº¤æ˜“agenté¡¹ç›®
- [ ] æœ‰å®Œæ•´çš„GitHub portfolio
- [ ] å†™è¿‡æŠ€æœ¯æ–‡æ¡£å’Œblog
- [ ] èƒ½demoä½ çš„é¡¹ç›®
- [ ] å‡†å¤‡å¥½é¢è¯•è®²è§£

---

## ğŸš€ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

### ä»Šå¤©å°±å¼€å§‹ (Day 1 è¡ŒåŠ¨æ¸…å•)

#### ç¬¬ä¸€æ­¥: ç¯å¢ƒæ­å»º (30åˆ†é’Ÿ)

```bash
# 1. åˆ›å»ºå­¦ä¹ æ–‡ä»¶å¤¹
mkdir -p ~/learning-ai
cd ~/learning-ai

# 2. åˆ›å»ºPythonè™šæ‹Ÿç¯å¢ƒ
python3 -m venv venv
source venv/bin/activate  # Mac/Linux
# æˆ– venv\Scripts\activate  # Windows

# 3. å®‰è£…PyTorch (æ ¹æ®ä½ çš„ç³»ç»Ÿé€‰æ‹©)
# Mac (Apple Silicon):
pip3 install torch torchvision torchaudio

# Windows/Linux (CUDA):
# è®¿é—® https://pytorch.org/get-started/locally/ é€‰æ‹©åˆé€‚çš„ç‰ˆæœ¬

# éªŒè¯å®‰è£…
python3 -c "import torch; print(f'PyTorch version: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}')"

# 4. å®‰è£…å…¶ä»–ä¾èµ–
pip install numpy matplotlib jupyter notebook

# 5. åˆ›å»ºGitHub repo
git init
echo "# My AI Learning Journey" > README.md
echo "venv/" > .gitignore
echo "*.pyc" >> .gitignore
echo "__pycache__/" >> .gitignore
git add .
git commit -m "Initial commit"

# åœ¨GitHubä¸Šåˆ›å»ºrepoå¹¶push
git remote add origin [your-repo-url]
git push -u origin main
```

#### ç¬¬äºŒæ­¥: Week 0 Day 1ä»»åŠ¡ (2å°æ—¶)

**ä»»åŠ¡1: PyTorch 60åˆ†é’Ÿæ•™ç¨‹** (1å°æ—¶)
```bash
# åˆ›å»ºDay 1æ–‡ä»¶å¤¹
mkdir week-0-pytorch/day-1
cd week-0-pytorch/day-1

# åˆ›å»ºJupyter notebook
jupyter notebook
# æˆ–è€…
code pytorch_basics.py  # å¦‚æœç”¨VSCode
```

**åœ¨notebook/pyæ–‡ä»¶é‡Œå®Œæˆ**:
```python
# File: pytorch_basics.py
import torch
import numpy as np

print("=" * 50)
print("Day 1: PyTorch Basics")
print("=" * 50)

# Task 1: Create tensors
print("\n1. Creating Tensors")
x = torch.tensor([1, 2, 3])
print(f"1D tensor: {x}")

y = torch.randn(3, 4)
print(f"Random 2D tensor:\n{y}")

z = torch.zeros(2, 3)
print(f"Zeros tensor:\n{z}")

# Task 2: Tensor operations
print("\n2. Tensor Operations")
a = torch.tensor([1.0, 2.0, 3.0])
b = torch.tensor([4.0, 5.0, 6.0])

print(f"a + b = {a + b}")
print(f"a * b = {a * b}")
print(f"a @ b = {torch.dot(a, b)}")  # dot product

# Task 3: Reshaping
print("\n3. Reshaping")
x = torch.randn(2, 3, 4)
print(f"Original shape: {x.shape}")
print(f"View as (2, 12): {x.view(2, 12).shape}")
print(f"View as (6, 4): {x.view(6, 4).shape}")
print(f"Transpose: {x.transpose(1, 2).shape}")

# Task 4: Autograd
print("\n4. Autograd")
x = torch.tensor([2.0], requires_grad=True)
y = x ** 2 + 3 * x
print(f"y = x^2 + 3x, where x = 2")
print(f"y = {y.item()}")

y.backward()
print(f"dy/dx = 2x + 3 = {x.grad.item()}")

# Task 5: GPU check
print("\n5. GPU Availability")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA device: {torch.cuda.get_device_name(0)}")

print("\nâœ… Day 1 completed!")
```

**è¿è¡Œå¹¶æäº¤**:
```bash
python pytorch_basics.py

# æäº¤åˆ°GitHub
git add .
git commit -m "Week 0 Day 1: PyTorch basics"
git push
```

**ä»»åŠ¡2: å¼€å§‹çœ‹è§†é¢‘** (1å°æ—¶)
- æ‰“å¼€: https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html
- çœ‹å‰30åˆ†é’Ÿ: "What is PyTorch?" å’Œ "Tensors"
- è¾¹çœ‹è¾¹åœ¨notebooké‡Œæ•²ä»£ç 

#### ç¬¬ä¸‰æ­¥: åˆ›å»ºå­¦ä¹ æ—¥å¿— (30åˆ†é’Ÿ)

åœ¨Notion/Obsidian/Markdownåˆ›å»ºå­¦ä¹ æ—¥å¿—:

```markdown
# AI Learning Journey

## Week 0: PyTorch Basics

### Day 1: 2024-12-18
**Time spent**: 2.5 hours
**What I learned**:
- PyTorch tensor basics
- Autograd mechanism
- Shape manipulation (view, transpose)

**Completed**:
- [x] Environment setup
- [x] PyTorch installation
- [x] Basic tensor operations
- [x] Autograd example

**Challenges**:
- Understanding broadcasting rules
- Remembering when to use .view() vs .reshape()

**Tomorrow's plan**:
- Complete PyTorch 60-min tutorial
- Start micrograd video

**Code**: [GitHub link]
```

### æœ¬å‘¨ç›®æ ‡ (Week 0å®Œæ•´è®¡åˆ’)

**Day 2**: 
- [ ] å®ŒæˆPyTorch 60åˆ†é’Ÿæ•™ç¨‹
- [ ] å¼€å§‹çœ‹microgradè§†é¢‘
- [ ] å®ç°ç®€å•çš„autograd example

**Day 3**:
- [ ] å®Œæˆmicrogradè§†é¢‘
- [ ] å®ç°çº¿æ€§å›å½’é¡¹ç›®

**Day 4**:
- [ ] å¼€å§‹MNISTé¡¹ç›®
- [ ] ç†è§£DataLoaderçš„ä½¿ç”¨

**Day 5**:
- [ ] å®ŒæˆMNISTåˆ†ç±»å™¨
- [ ] å­¦ä¹ Transformerä¸­çš„tensoræ“ä½œ

**Day 6**:
- [ ] å®ç°ç®€åŒ–çš„attentionæœºåˆ¶
- [ ] å¤ä¹ æœ¬å‘¨å†…å®¹

**Day 7**:
- [ ] GPUè®­ç»ƒå®è·µ
- [ ] æ¨¡å‹ä¿å­˜å’ŒåŠ è½½
- [ ] Week 0æ€»ç»“å’Œå¤ä¹ 

### æŒç»­è·Ÿè¸ª

**æ¯å¤©**:
1. æ›´æ–°å­¦ä¹ æ—¥å¿—
2. æäº¤ä»£ç åˆ°GitHub
3. å¦‚æœå¡ä½è¶…è¿‡1å°æ—¶ï¼Œç«‹åˆ»é—®æˆ‘

**æ¯å‘¨æ—¥**:
1. å†™æœ¬å‘¨æ€»ç»“
2. å®Œæˆå‘¨éªŒæ”¶é¡¹ç›®
3. è§„åˆ’ä¸‹å‘¨å­¦ä¹ 

**å’Œæˆ‘çš„äº’åŠ¨**:
- æ¯å¤©åˆ†äº«ä½ çš„è¿›åº¦
- é‡åˆ°é—®é¢˜ç«‹åˆ»é—®
- æƒ³è®¨è®ºæ¦‚å¿µéšæ—¶æ‰¾æˆ‘
- å®Œæˆé¡¹ç›®åç»™æˆ‘çœ‹demo

### è·å–å¸®åŠ©çš„æ–¹å¼

1. **å¡ä½äº†?** 
   - å…ˆGoogle 10åˆ†é’Ÿ
   - è¿˜æ˜¯ä¸è¡Œå°±é—®æˆ‘
   - æä¾›: é”™è¯¯ä¿¡æ¯ + ä»£ç  + ä½ çš„ç†è§£

2. **éœ€è¦ä»£ç review?** 
   - æŠŠGitHubé“¾æ¥å‘ç»™æˆ‘
   - æˆ‘ä¼šæä¾›åé¦ˆå’Œæ”¹è¿›å»ºè®®

3. **æƒ³è®¨è®ºé¡¹ç›®?** 
   - éšæ—¶brainstorm
   - æˆ‘å¸®ä½ è§„åˆ’å®ç°æ­¥éª¤

4. **é¢è¯•å‡†å¤‡?** 
   - Week 4åå¯ä»¥å¼€å§‹mock interview
   - æˆ‘æ¨¡æ‹Ÿé¢è¯•å®˜é—®ä½ TransformeråŸç†

### æ¿€åŠ±å’Œé‡Œç¨‹ç¢‘

**Week 0ç»“æŸ**: ğŸ‰
- æŒæ¡PyTorchåŸºç¡€
- å®ŒæˆMNISTåˆ†ç±»å™¨
- è§£é” "PyTorch Developer" æˆå°±

**Week 2ç»“æŸ**: ğŸš€
- å®ç°mini Transformer
- è§£é” "Transformer Implementor" æˆå°±
- å¯ä»¥å»é¢è¯•è®²hand-coded attention

**Week 4ç»“æŸ**: ğŸ’ª
- Fine-tunedè‡ªå·±çš„ç¬¬ä¸€ä¸ªLLM
- è§£é” "LLM Engineer" æˆå°±
- å¼€å§‹åšAIåº”ç”¨å¼€å‘

**Week 13ç»“æŸ**: ğŸ†
- å®Œæˆé‡åŒ–äº¤æ˜“Agentç³»ç»Ÿ
- æœ‰å®Œæ•´çš„GitHub portfolio
- è§£é” "AI Agent Master" æˆå°±
- å‡†å¤‡å¥½å»é¢è¯•äº†ï¼

---

## ğŸ“š æœ€åçš„è¯

è¿™æ˜¯ä¸€æ¡ä»**åº•å±‚åŸç†åˆ°å®æˆ˜åº”ç”¨**çš„å®Œæ•´è·¯å¾„ã€‚13å‘¨åï¼Œä½ å°†:

âœ… **æŠ€æœ¯èƒ½åŠ›**:
- ç†è§£Transformerå¦‚ä½•å·¥ä½œ
- èƒ½æ‰‹å†™æ ¸å¿ƒç»„ä»¶ (é¢è¯•åŠ åˆ†)
- æŒæ¡LLMçš„è®­ç»ƒå’Œä½¿ç”¨
- æ„å»ºproduction-readyçš„AI agents
- PyTorchç†Ÿç»ƒä½¿ç”¨

âœ… **é¡¹ç›®ç»éªŒ**:
- å®Œæ•´çš„portfolioé¡¹ç›®
- GitHubä¸Šæœ‰çœŸå®ä»£ç 
- å¯demoçš„AIåº”ç”¨
- æŠ€æœ¯blogæ–‡ç« 

âœ… **èŒä¸šä¼˜åŠ¿**:
- åœ¨AI+FinTeché¢†åŸŸå»ºç«‹ç‹¬ç‰¹ä¼˜åŠ¿
- æ—¢æ‡‚åº•å±‚åŸç†åˆèƒ½åšåº”ç”¨
- ç»“åˆé‡åŒ–äº¤æ˜“domain knowledge
- å›½å†…å¤§å‚AIå²—çš„å®Œæ•´å‡†å¤‡

**è®°ä½**: ä½ çš„èƒŒæ™¯ï¼ˆCS + å…¨æ ˆ + é‡åŒ–äº¤æ˜“ï¼‰æ˜¯å·¨å¤§çš„ä¼˜åŠ¿ã€‚å¾ˆå°‘æœ‰äººåŒæ—¶å…·å¤‡engineeringèƒ½åŠ›å’Œdomain knowledgeã€‚å……åˆ†åˆ©ç”¨è¿™ä¸ªä¼˜åŠ¿ï¼Œæ„å»ºæœ‰å®é™…ä»·å€¼çš„AIåº”ç”¨ã€‚

**æœ€é‡è¦çš„**: 
- ğŸš€ **å¼€å§‹æ¯”å®Œç¾æ›´é‡è¦**
- ğŸ’ª **åšæŒæ¯”èªæ˜æ›´é‡è¦**
- ğŸ¯ **å®è·µæ¯”ç†è®ºæ›´é‡è¦**

### ä»Šå¤©å°±å¼€å§‹ç¬¬ä¸€æ­¥ï¼

ç°åœ¨ç«‹åˆ»æ‰§è¡ŒDay 1çš„ç¯å¢ƒæ­å»ºï¼Œ30åˆ†é’Ÿåä½ å°±å¯ä»¥å†™ç¬¬ä¸€è¡ŒPyTorchä»£ç äº†ï¼

æœ‰ä»»ä½•é—®é¢˜éšæ—¶é—®æˆ‘ï¼Œæˆ‘ä¼šä¸€è·¯é™ªä½ å­¦ä¹ ï¼

Good luck! ğŸš€

---

## ğŸ“ é™„å½•: å¿«é€Ÿå‚è€ƒ

### PyTorchå¸¸ç”¨æ“ä½œé€ŸæŸ¥

```python
# Tensoråˆ›å»º
torch.tensor([1, 2, 3])
torch.randn(3, 4)
torch.zeros(2, 3)
torch.ones(2, 3)
torch.arange(0, 10, 2)

# Shapeæ“ä½œ
x.shape / x.size()
x.view(new_shape)
x.reshape(new_shape)
x.transpose(dim0, dim1)
x.unsqueeze(dim)
x.squeeze(dim)

# æ•°å­¦æ“ä½œ
x + y, x * y, x @ y
x.sum(), x.mean(), x.max()
torch.matmul(x, y)
torch.softmax(x, dim=-1)

# Autograd
x.requires_grad = True
y.backward()
x.grad

# GPU
x.to('cuda')
x.cuda()
x.cpu()

# æ¨¡å‹ç›¸å…³
model.parameters()
model.train() / model.eval()
model.state_dict()
model.load_state_dict()
```

### Transformerç»´åº¦é€ŸæŸ¥

```python
# å¸¸è§ç»´åº¦
batch_size = 32
seq_len = 128
d_model = 512
num_heads = 8
d_k = d_model // num_heads  # 64

# Input
x: [batch, seq_len, d_model]

# Multi-head attention
Q, K, V: [batch, num_heads, seq_len, d_k]
scores: [batch, num_heads, seq_len, seq_len]
output: [batch, seq_len, d_model]

# Feed-forward
input: [batch, seq_len, d_model]
hidden: [batch, seq_len, d_ff]  # d_ffé€šå¸¸æ˜¯4*d_model
output: [batch, seq_len, d_model]
```

### è°ƒè¯•æŠ€å·§é€ŸæŸ¥

```python
# æ£€æŸ¥shape
print(f"x.shape: {x.shape}")

# æ£€æŸ¥å€¼
print(f"x.min(): {x.min()}, x.max(): {x.max()}")

# æ£€æŸ¥æ¢¯åº¦
print(f"x.grad: {x.grad}")

# æ£€æŸ¥NaN
assert not torch.isnan(x).any()

# è®¾å¤‡
print(f"x.device: {x.device}")
```

---

*æ–‡æ¡£ç‰ˆæœ¬: v2.0 (åŒ…å«å®Œæ•´PyTorch Week 0)*
*ç»´æŠ¤è€…: Claude*
*æœ€åæ›´æ–°: 2024-12-18*
*å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œéšæ—¶åé¦ˆï¼*
