# AIå·¥ç¨‹åŒ–å¿«é€Ÿå®æˆ˜è·¯çº¿ï¼ˆ4-6å‘¨ï¼‰

> ä¸“æ³¨å·¥ç¨‹è½åœ°èƒ½åŠ› - RAG + Fine-tuning + Agentç¼–æ’
> 
> ä¸ºJasonå®šåˆ¶ - å¿«é€ŸæŒæ¡AIåº”ç”¨å¼€å‘
> 
> æœ€åæ›´æ–°: 2025-01-05

---

## ğŸ“‹ ç›®å½•

1. [æ ¸å¿ƒæ€è·¯](#æ ¸å¿ƒæ€è·¯)
2. [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
3. [Week 1-2: RAGç³»ç»Ÿå®æˆ˜](#week-1-2-ragç³»ç»Ÿå®æˆ˜)
4. [Week 3: Fine-tuningå®æˆ˜](#week-3-fine-tuningå®æˆ˜)
5. [Week 4-5: Agentå·¥ä½œæµç¼–æ’](#week-4-5-agentå·¥ä½œæµç¼–æ’)
6. [Week 6: é¡¹ç›®æ•´åˆ](#week-6-é¡¹ç›®æ•´åˆ)
7. [å¿«é€Ÿå‚è€ƒ](#å¿«é€Ÿå‚è€ƒ)

---

## ğŸ¯ æ ¸å¿ƒæ€è·¯

```
è·³è¿‡ç†è®ºæ·±æŒ– â†’ ç›´æ¥å®æˆ˜
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âŒ ä¸å­¦ï¼šæ‰‹æ’¸Transformer
âŒ ä¸å­¦ï¼šPyTorchåº•å±‚
âŒ ä¸å­¦ï¼šæ•°å­¦æ¨å¯¼

âœ… åªå­¦ï¼šå·¥ç¨‹åŒ–è½åœ°
âœ… é‡ç‚¹ï¼šRAGç³»ç»Ÿ
âœ… é‡ç‚¹ï¼šFine-tuningå®æˆ˜
âœ… é‡ç‚¹ï¼šAgentå·¥ä½œæµç¼–æ’
âœ… ç›®æ ‡ï¼š2ä¸ªproduction-readyé¡¹ç›®
```

### å­¦ä¹ åŸåˆ™

1. **å¤åˆ¶ç²˜è´´ä¼˜å…ˆ** - ç”¨ç°æˆçš„ä»£ç æ¨¡æ¿å¿«é€Ÿè¿­ä»£
2. **å¤Ÿç”¨å°±è¡Œ** - ä¸æ·±ç©¶åŸç†ï¼Œèƒ½ç”¨å°±è¡Œ
3. **æ¯å‘¨ä¸€ä¸ªé¡¹ç›®** - ç«‹åˆ»èƒ½demoçš„æˆæœ
4. **é‡åˆ°é—®é¢˜ç«‹åˆ»é—®** - ä¸è¦å¡è¶…è¿‡1å°æ—¶

---

## ğŸ› ï¸ ç¯å¢ƒå‡†å¤‡

### å®‰è£…ä¾èµ–ï¼ˆ15åˆ†é’Ÿï¼‰

```bash
# 1. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Mac/Linux
# æˆ– venv\Scripts\activate  # Windows

# 2. å®‰è£…æ ¸å¿ƒåŒ…
pip install langchain langchain-openai langgraph

# 3. å®‰è£…è¾…åŠ©åŒ…
pip install chromadb streamlit fastapi uvicorn
pip install yfinance pandas numpy matplotlib

# 4. å®‰è£…Fine-tuningç›¸å…³ï¼ˆWeek 3éœ€è¦ï¼‰
pip install transformers datasets accelerate

# 5. éªŒè¯å®‰è£…
python -c "
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph
print('âœ… å®‰è£…æˆåŠŸ!')
"
```

### API Keyså‡†å¤‡

```bash
# å¿…éœ€ï¼ˆäºŒé€‰ä¸€ï¼‰
export OPENAI_API_KEY="your-openai-key"
# æˆ–
export ANTHROPIC_API_KEY="your-claude-key"

# å¯é€‰
export NEWS_API_KEY="your-news-api-key"
```

### åˆ›å»ºé¡¹ç›®ç»“æ„

```bash
mkdir ai-stock-research
cd ai-stock-research

# é¡¹ç›®ç»“æ„
ai-stock-research/
â”œâ”€â”€ week1-rag/           # RAGç³»ç»Ÿ
â”œâ”€â”€ week3-finetuning/    # Fine-tuning
â”œâ”€â”€ week4-agent/         # Agentç¼–æ’
â”œâ”€â”€ final-project/       # æœ€ç»ˆé¡¹ç›®
â””â”€â”€ README.md
```

---

## ğŸ“š Week 1-2: RAGç³»ç»Ÿå®æˆ˜

**ç›®æ ‡**: å¿«é€Ÿæ„å»ºèƒ½ç”¨çš„RAGç³»ç»Ÿ

### Day 1-2: LangChain RAGé€Ÿæˆï¼ˆ4å°æ—¶ï¼‰

#### èµ„æº1: LangChain RAGå®˜æ–¹æ•™ç¨‹
- **é“¾æ¥**: https://python.langchain.com/docs/tutorials/rag/
- **æ—¶é•¿**: 2å°æ—¶
- **é‡ç‚¹**: Document loading, Splitting, Embedding, Retrieval

#### ç¬¬ä¸€ä¸ªRAGåŸå‹ï¼ˆ30åˆ†é’Ÿè·‘é€šï¼‰

```python
# File: simple_rag.py
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain_community.document_loaders import TextLoader

def create_rag_system():
    # 1. åˆ›å»ºæµ‹è¯•æ–‡æ¡£
    with open("trading_notes.txt", "w") as f:
        f.write("""
        NVDA Trading Strategy:
        - Entry: When RSI < 30
        - Exit: When RSI > 70
        - Stop loss: 5% below entry
        
        TSLA Analysis:
        - Strong momentum in Q3 2024
        - Revenue beat expectations
        """)
    
    # 2. åŠ è½½æ–‡æ¡£
    loader = TextLoader("trading_notes.txt")
    documents = loader.load()
    
    # 3. åˆ‡åˆ†
    splitter = RecursiveCharacterTextSplitter(chunk_size=200)
    chunks = splitter.split_documents(documents)
    
    # 4. åˆ›å»ºå‘é‡åº“
    vectorstore = Chroma.from_documents(
        documents=chunks,
        embedding=OpenAIEmbeddings()
    )
    
    # 5. åˆ›å»ºQAé“¾
    qa = RetrievalQA.from_chain_type(
        llm=ChatOpenAI(),
        retriever=vectorstore.as_retriever()
    )
    
    return qa

# æµ‹è¯•
if __name__ == "__main__":
    qa = create_rag_system()
    
    question = "What's my NVDA trading strategy?"
    answer = qa.invoke(question)
    print(f"Q: {question}")
    print(f"A: {answer['result']}")
```

**è¿è¡Œ**:
```bash
export OPENAI_API_KEY="your-key"
python simple_rag.py
```

#### Day 1-2ä»»åŠ¡æ¸…å•
- [ ] è·‘é€šä¸Šé¢çš„ä»£ç ï¼ˆ30åˆ†é’Ÿï¼‰
- [ ] ç”¨ä½ çš„é‡åŒ–äº¤æ˜“ç¬”è®°æ›¿æ¢æµ‹è¯•æ–‡æ¡£
- [ ] æµ‹è¯•3-5ä¸ªä¸åŒçš„é—®é¢˜
- [ ] è§‚å¯Ÿå›ç­”è´¨é‡

### Day 3-5: RAGä¼˜åŒ–å®æˆ˜ï¼ˆ6å°æ—¶ï¼‰

#### ä¼˜åŒ–ç‚¹1: æ›´å¥½çš„æ–‡æ¡£å¤„ç†

```python
# æ”¯æŒPDFæ–‡æ¡£
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader

# æ–¹æ³•1: å•ä¸ªPDF
loader = PyPDFLoader("trading_strategy.pdf")
documents = loader.load()

# æ–¹æ³•2: æ•´ä¸ªæ–‡ä»¶å¤¹
loader = DirectoryLoader(
    "./trading_notes/",
    glob="**/*.pdf",
    loader_cls=PyPDFLoader
)
documents = loader.load()

print(f"Loaded {len(documents)} documents")
```

#### ä¼˜åŒ–ç‚¹2: æ··åˆæ£€ç´¢ï¼ˆHybrid Searchï¼‰

```python
# ç»“åˆè¯­ä¹‰æœç´¢å’Œå…³é”®è¯æœç´¢
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever

# è¯­ä¹‰æ£€ç´¢
vector_retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# å…³é”®è¯æ£€ç´¢
bm25_retriever = BM25Retriever.from_documents(documents)
bm25_retriever.k = 3

# æ··åˆ
ensemble_retriever = EnsembleRetriever(
    retrievers=[vector_retriever, bm25_retriever],
    weights=[0.7, 0.3]  # 70%è¯­ä¹‰ï¼Œ30%å…³é”®è¯
)

# ä½¿ç”¨æ··åˆæ£€ç´¢
qa = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(),
    retriever=ensemble_retriever
)
```

#### ä¼˜åŒ–ç‚¹3: Re-rankingæå‡å‡†ç¡®åº¦

```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

# ç”¨LLMé‡æ–°æ’åºæ£€ç´¢ç»“æœ
compressor = LLMChainExtractor.from_llm(llm)

compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=vector_retriever
)
```

### Day 6-7: æ·»åŠ Webç•Œé¢ï¼ˆ4å°æ—¶ï¼‰

#### Streamlitç•Œé¢ï¼ˆ1å°æ—¶æ­å»ºï¼‰

```python
# File: app.py
import streamlit as st
from simple_rag import create_rag_system

st.set_page_config(page_title="Stock Research Assistant", page_icon="ğŸ“ˆ")

st.title("ğŸ“ˆ Stock Research Assistant")

# ä¾§è¾¹æ 
with st.sidebar:
    st.header("é…ç½®")
    uploaded_file = st.file_uploader(
        "ä¸Šä¼ äº¤æ˜“ç¬”è®° (PDF/TXT)",
        type=['pdf', 'txt']
    )
    
    if uploaded_file:
        st.success("æ–‡æ¡£å·²ä¸Šä¼ !")

# ä¸»ç•Œé¢
st.markdown("### ğŸ’¬ é—®ç­”")

# åˆå§‹åŒ–session state
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.qa = create_rag_system()

# æ˜¾ç¤ºèŠå¤©å†å²
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¯¢é—®ä½ çš„äº¤æ˜“ç­–ç•¥..."):
    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # è·å–AIå›ç­”
    with st.chat_message("assistant"):
        with st.spinner("æ€è€ƒä¸­..."):
            response = st.session_state.qa.invoke(prompt)
            answer = response['result']
            st.markdown(answer)
    
    # ä¿å­˜AIå›ç­”
    st.session_state.messages.append({"role": "assistant", "content": answer})
```

**è¿è¡Œ**:
```bash
streamlit run app.py
```

### Week 1-2äº¤ä»˜ç‰©

**å®Œæˆæ ‡å‡†**:
- âœ… RAGç³»ç»Ÿèƒ½æ­£ç¡®æ£€ç´¢æ–‡æ¡£
- âœ… æ”¯æŒPDFå’ŒTXTæ–‡ä»¶
- âœ… æœ‰ç®€æ´çš„Webç•Œé¢
- âœ… å›ç­”å‡†ç¡®ä¸”æœ‰æ¥æº
- âœ… GitHub repo + README
- âœ… 5åˆ†é’Ÿdemoè§†é¢‘

**éªŒæ”¶é—®é¢˜**:
1. "RAGçš„ä¸‰ä¸ªæ ¸å¿ƒæ­¥éª¤æ˜¯ä»€ä¹ˆï¼Ÿ"
2. "å±•ç¤ºä½ çš„RAGç³»ç»ŸæŸ¥è¯¢æ•ˆæœ"
3. "å¦‚ä½•æå‡æ£€ç´¢å‡†ç¡®åº¦ï¼Ÿ"

---

## ğŸ¤– Week 3: Fine-tuningå®æˆ˜

**ç›®æ ‡**: å¿«é€ŸæŒæ¡æ¨¡å‹å¾®è°ƒ

### Day 1-2: HuggingFaceé€Ÿæˆï¼ˆ4å°æ—¶ï¼‰

#### èµ„æº1: HuggingFace Quick Tour
- **é“¾æ¥**: https://huggingface.co/docs/transformers/quicktour
- **æ—¶é•¿**: 1å°æ—¶
- **é‡ç‚¹**: åŠ è½½æ¨¡å‹ã€ä½¿ç”¨pipeline

#### èµ„æº2: Fine-tuningæ•™ç¨‹
- **é“¾æ¥**: https://huggingface.co/docs/transformers/training
- **æ—¶é•¿**: 2å°æ—¶
- **é‡ç‚¹**: Trainer APIã€è®­ç»ƒé…ç½®

#### ç¬¬ä¸€ä¸ªFine-tuningä¾‹å­ï¼ˆ1å°æ—¶è·‘é€šï¼‰

```python
# File: finetune_sentiment.py
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments
)
from datasets import load_dataset

# 1. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model_name = "distilbert-base-uncased"
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=3  # æ­£é¢ã€è´Ÿé¢ã€ä¸­æ€§
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 2. åŠ è½½æ•°æ®é›†ï¼ˆé‡‘èæƒ…ç»ªæ•°æ®ï¼‰
dataset = load_dataset("financial_phrasebank", "sentences_allagree")

# 3. é¢„å¤„ç†
def tokenize_function(examples):
    return tokenizer(
        examples["sentence"],
        padding="max_length",
        truncation=True
    )

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# 4. è®­ç»ƒé…ç½®
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=100,
    evaluation_strategy="steps",
    eval_steps=500,
)

# 5. åˆ›å»ºTrainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
)

# 6. å¼€å§‹è®­ç»ƒ
trainer.train()

# 7. ä¿å­˜æ¨¡å‹
model.save_pretrained("./financial-sentiment-model")
tokenizer.save_pretrained("./financial-sentiment-model")

print("âœ… Fine-tuningå®Œæˆ!")
```

### Day 3-5: å®æˆ˜Fine-tuningé¡¹ç›®ï¼ˆ8å°æ—¶ï¼‰

#### é¡¹ç›®: é‡‘èæƒ…ç»ªåˆ†ææ¨¡å‹

**æ•°æ®å‡†å¤‡**:
```python
# æ–¹æ³•1: ä½¿ç”¨ç°æˆæ•°æ®é›†
from datasets import load_dataset

dataset = load_dataset("financial_phrasebank", "sentences_allagree")

# æ–¹æ³•2: è‡ªå·±æ ‡æ³¨æ•°æ®
import pandas as pd

data = {
    'text': [
        "NVIDIA reports record revenue",
        "Stock market crashes amid recession fears",
        "Company announces layoffs"
    ],
    'label': [2, 0, 0]  # 0=è´Ÿé¢, 1=ä¸­æ€§, 2=æ­£é¢
}

df = pd.DataFrame(data)
df.to_csv("my_financial_sentiment.csv", index=False)
```

**è®­ç»ƒè„šæœ¬**ï¼ˆå®Œæ•´ç‰ˆï¼‰:
```python
# File: train_sentiment.py
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments,
    EarlyStoppingCallback
)
from datasets import load_dataset
import numpy as np
from sklearn.metrics import accuracy_score, f1_score

# åŠ è½½æ•°æ®
dataset = load_dataset("financial_phrasebank", "sentences_allagree")

# åŠ è½½æ¨¡å‹
model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=3
)

# æ•°æ®é¢„å¤„ç†
def preprocess(examples):
    return tokenizer(
        examples["sentence"],
        truncation=True,
        padding="max_length",
        max_length=128
    )

tokenized = dataset.map(preprocess, batched=True)

# è¯„ä¼°æŒ‡æ ‡
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    
    return {
        'accuracy': accuracy_score(labels, predictions),
        'f1': f1_score(labels, predictions, average='weighted')
    }

# è®­ç»ƒé…ç½®
training_args = TrainingArguments(
    output_dir="./sentiment-model",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=100,
    evaluation_strategy="steps",
    eval_steps=500,
    save_strategy="steps",
    save_steps=500,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
)

# è®­ç»ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized["train"],
    eval_dataset=tokenized["validation"],
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
)

# å¼€å§‹è®­ç»ƒ
print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
trainer.train()

# è¯„ä¼°
print("\nğŸ“Š è¯„ä¼°ç»“æœ:")
eval_results = trainer.evaluate()
print(eval_results)

# ä¿å­˜
trainer.save_model("./final-sentiment-model")
print("\nâœ… æ¨¡å‹å·²ä¿å­˜åˆ° ./final-sentiment-model")
```

#### ä½¿ç”¨Fine-tunedæ¨¡å‹

```python
# File: use_model.py
from transformers import pipeline

# åŠ è½½æ¨¡å‹
classifier = pipeline(
    "sentiment-analysis",
    model="./final-sentiment-model"
)

# æµ‹è¯•
texts = [
    "NVIDIA stock surges on strong earnings",
    "Market crashes amid economic uncertainty",
    "Company reports stable quarterly results"
]

results = classifier(texts)
for text, result in zip(texts, results):
    print(f"Text: {text}")
    print(f"Sentiment: {result['label']}, Confidence: {result['score']:.2f}\n")
```

### Day 6-7: éƒ¨ç½²ä¸ºAPIï¼ˆ4å°æ—¶ï¼‰

```python
# File: api.py
from fastapi import FastAPI
from pydantic import BaseModel
from transformers import pipeline

app = FastAPI()

# åŠ è½½æ¨¡å‹
classifier = pipeline("sentiment-analysis", model="./final-sentiment-model")

class TextRequest(BaseModel):
    text: str

@app.post("/analyze")
async def analyze_sentiment(request: TextRequest):
    result = classifier(request.text)[0]
    return {
        "text": request.text,
        "sentiment": result['label'],
        "confidence": result['score']
    }

@app.get("/")
async def root():
    return {"message": "Financial Sentiment API"}

# è¿è¡Œ: uvicorn api:app --reload
```

### Week 3äº¤ä»˜ç‰©

**å®Œæˆæ ‡å‡†**:
- âœ… Fine-tunedé‡‘èæƒ…ç»ªåˆ†ææ¨¡å‹
- âœ… å‡†ç¡®ç‡>80%
- âœ… éƒ¨ç½²ä¸ºAPI
- âœ… æ¨¡å‹ä¸Šä¼ åˆ°HuggingFace Hubï¼ˆå¯é€‰ï¼‰
- âœ… æµ‹è¯•æŠ¥å‘Š

**éªŒæ”¶é—®é¢˜**:
1. "å±•ç¤ºä½ Fine-tunedçš„æ¨¡å‹æ•ˆæœ"
2. "è®­ç»ƒäº†å¤šå°‘epochï¼Ÿå‡†ç¡®ç‡å¦‚ä½•ï¼Ÿ"
3. "å¦‚ä½•åœ¨HuggingFaceä¸ŠFine-tuneæ¨¡å‹ï¼Ÿ"

---

## ğŸ¤– Week 4-5: Agentå·¥ä½œæµç¼–æ’

**ç›®æ ‡**: ç”¨LangGraphæ„å»ºMulti-agentç³»ç»Ÿ

### Day 1: LangChainå‰ç½®çŸ¥è¯†ï¼ˆ3å°æ—¶ï¼‰

#### å¿…å­¦æ¦‚å¿µï¼ˆä»…æ­¤è€Œå·²ï¼‰

**1. LLMè°ƒç”¨ï¼ˆ15åˆ†é’Ÿï¼‰**:
```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4", temperature=0)
response = llm.invoke("Hello!")
print(response.content)
```

**2. Toolå®šä¹‰ï¼ˆ30åˆ†é’Ÿï¼‰**:
```python
from langchain.tools import tool

@tool
def get_stock_price(ticker: str) -> str:
    """Get current stock price for a ticker."""
    import yfinance as yf
    stock = yf.Ticker(ticker)
    price = stock.info.get('currentPrice', 'N/A')
    return f"{ticker}: ${price}"

@tool
def calculate_rsi(ticker: str) -> str:
    """Calculate RSI indicator."""
    # ç®€åŒ–å®ç°
    return f"RSI for {ticker}: 45.5"

# æµ‹è¯•
result = get_stock_price.invoke({"ticker": "AAPL"})
print(result)
```

**3. Simple Agentï¼ˆ1å°æ—¶ï¼‰**:
```python
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain.prompts import ChatPromptTemplate

tools = [get_stock_price, calculate_rsi]

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a stock analysis assistant."),
    ("human", "{input}"),
    ("placeholder", "{agent_scratchpad}"),
])

agent = create_tool_calling_agent(llm, tools, prompt)
executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

# æµ‹è¯•
result = executor.invoke({"input": "What's AAPL price and RSI?"})
print(result["output"])
```

### Day 2-3: LangGraphæ ¸å¿ƒï¼ˆ6å°æ—¶ï¼‰

#### èµ„æº: LangGraphå®˜æ–¹æ•™ç¨‹
- **é“¾æ¥**: https://langchain-ai.github.io/langgraph/tutorials/introduction/
- **æ—¶é•¿**: 3å°æ—¶
- **é‡ç‚¹**: StateGraph, Node, Edge, Conditional routing

#### LangGraphæ ¸å¿ƒæ¦‚å¿µ

**æ¦‚å¿µ1: Stateï¼ˆçŠ¶æ€ï¼‰**:
```python
from typing import TypedDict

class AgentState(TypedDict):
    messages: list      # å¯¹è¯å†å²
    ticker: str         # è‚¡ç¥¨ä»£ç 
    data: dict          # æ•°æ®
    analysis: str       # åˆ†æç»“æœ
    next_step: str      # ä¸‹ä¸€æ­¥åŠ¨ä½œ
```

**æ¦‚å¿µ2: Nodeï¼ˆèŠ‚ç‚¹ï¼‰= å‡½æ•°**:
```python
def research_node(state: AgentState) -> AgentState:
    """è·å–è‚¡ç¥¨æ•°æ®"""
    ticker = state["ticker"]
    # è°ƒç”¨APIè·å–æ•°æ®
    data = fetch_stock_data(ticker)
    return {"data": data}

def analysis_node(state: AgentState) -> AgentState:
    """åˆ†ææ•°æ®"""
    analysis = analyze_stock(state["data"])
    return {"analysis": analysis}
```

**æ¦‚å¿µ3: Graphï¼ˆå›¾ï¼‰= å·¥ä½œæµ**:
```python
from langgraph.graph import StateGraph, END

# åˆ›å»ºå·¥ä½œæµ
workflow = StateGraph(AgentState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("research", research_node)
workflow.add_node("analysis", analysis_node)

# è®¾ç½®å…¥å£
workflow.set_entry_point("research")

# æ·»åŠ è¾¹ï¼ˆå®šä¹‰æµç¨‹ï¼‰
workflow.add_edge("research", "analysis")
workflow.add_edge("analysis", END)

# ç¼–è¯‘
app = workflow.compile()

# è¿è¡Œ
result = app.invoke({
    "ticker": "NVDA",
    "messages": []
})
```

#### ç¬¬ä¸€ä¸ªå®Œæ•´LangGraph Agentï¼ˆ2å°æ—¶ï¼‰

```python
# File: langgraph_agent.py
from langgraph.graph import StateGraph, END
from typing import TypedDict, Literal
from langchain_openai import ChatOpenAI
from langchain.tools import tool
import yfinance as yf

# 1. å®šä¹‰çŠ¶æ€
class TradingState(TypedDict):
    ticker: str
    price: float
    rsi: float
    recommendation: str
    messages: list

# 2. å®šä¹‰å·¥å…·
@tool
def fetch_price(ticker: str) -> float:
    """Fetch current stock price."""
    stock = yf.Ticker(ticker)
    return stock.info.get('currentPrice', 0)

@tool
def calculate_rsi(ticker: str) -> float:
    """Calculate RSI indicator."""
    stock = yf.Ticker(ticker)
    hist = stock.history(period="3mo")
    
    # ç®€åŒ–RSIè®¡ç®—
    delta = hist['Close'].diff()
    gain = delta.where(delta > 0, 0).rolling(14).mean()
    loss = -delta.where(delta < 0, 0).rolling(14).mean()
    rs = gain / loss
    rsi = 100 - (100 / (1 + rs))
    
    return float(rsi.iloc[-1])

# 3. å®šä¹‰èŠ‚ç‚¹
def fetch_data_node(state: TradingState) -> TradingState:
    ticker = state["ticker"]
    price = fetch_price.invoke({"ticker": ticker})
    rsi = calculate_rsi.invoke({"ticker": ticker})
    
    return {
        "price": price,
        "rsi": rsi,
        "messages": state["messages"] + [f"Fetched data for {ticker}"]
    }

def analyze_node(state: TradingState) -> TradingState:
    llm = ChatOpenAI(model="gpt-4", temperature=0)
    
    prompt = f"""
    Analyze {state['ticker']}:
    - Current Price: ${state['price']}
    - RSI: {state['rsi']}
    
    Provide a trading recommendation (BUY/HOLD/SELL) with reasoning.
    """
    
    analysis = llm.invoke(prompt).content
    
    return {
        "recommendation": analysis,
        "messages": state["messages"] + ["Analysis completed"]
    }

# 4. æ¡ä»¶åˆ¤æ–­
def should_analyze(state: TradingState) -> Literal["analyze", "end"]:
    """å†³å®šæ˜¯å¦ç»§ç»­åˆ†æ"""
    if state.get("price", 0) > 0:
        return "analyze"
    return "end"

# 5. æ„å»ºå·¥ä½œæµ
workflow = StateGraph(TradingState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("fetch_data", fetch_data_node)
workflow.add_node("analyze", analyze_node)

# è®¾ç½®æµç¨‹
workflow.set_entry_point("fetch_data")
workflow.add_conditional_edges(
    "fetch_data",
    should_analyze,
    {
        "analyze": "analyze",
        "end": END
    }
)
workflow.add_edge("analyze", END)

# ç¼–è¯‘
app = workflow.compile()

# æµ‹è¯•
if __name__ == "__main__":
    result = app.invoke({
        "ticker": "NVDA",
        "messages": [],
        "price": 0,
        "rsi": 0,
        "recommendation": ""
    })
    
    print("\n" + "="*60)
    print("åˆ†æç»“æœ:")
    print("="*60)
    print(f"è‚¡ç¥¨: {result['ticker']}")
    print(f"ä»·æ ¼: ${result['price']}")
    print(f"RSI: {result['rsi']:.2f}")
    print(f"\næ¨è:\n{result['recommendation']}")
```

### Day 4-7: Multi-Agentç³»ç»Ÿï¼ˆ10å°æ—¶ï¼‰

#### æ„å»ºå¤šAgentåä½œç³»ç»Ÿ

```python
# File: multi_agent.py
from langgraph.graph import StateGraph, END
from typing import TypedDict
from langchain_openai import ChatOpenAI

class MultiAgentState(TypedDict):
    ticker: str
    research_data: dict
    technical_analysis: dict
    fundamental_analysis: dict
    final_report: str
    messages: list

# === Agent 1: Research Agent ===
def research_agent(state: MultiAgentState) -> MultiAgentState:
    """æ”¶é›†æ•°æ®"""
    ticker = state["ticker"]
    
    # è·å–å¤šç§æ•°æ®
    import yfinance as yf
    stock = yf.Ticker(ticker)
    
    research_data = {
        "info": stock.info,
        "history": stock.history(period="1mo"),
        "news": stock.news[:5] if hasattr(stock, 'news') else []
    }
    
    return {
        "research_data": research_data,
        "messages": state["messages"] + ["Research completed"]
    }

# === Agent 2: Technical Analysis Agent ===
def technical_agent(state: MultiAgentState) -> MultiAgentState:
    """æŠ€æœ¯åˆ†æ"""
    hist = state["research_data"]["history"]
    
    # è®¡ç®—æŒ‡æ ‡
    analysis = {
        "sma_20": hist['Close'].rolling(20).mean().iloc[-1],
        "sma_50": hist['Close'].rolling(50).mean().iloc[-1],
        "current_price": hist['Close'].iloc[-1],
        "volume": hist['Volume'].iloc[-1]
    }
    
    return {
        "technical_analysis": analysis,
        "messages": state["messages"] + ["Technical analysis done"]
    }

# === Agent 3: Fundamental Analysis Agent ===
def fundamental_agent(state: MultiAgentState) -> MultiAgentState:
    """åŸºæœ¬é¢åˆ†æ"""
    info = state["research_data"]["info"]
    
    analysis = {
        "pe_ratio": info.get('forwardPE', 'N/A'),
        "market_cap": info.get('marketCap', 'N/A'),
        "revenue": info.get('totalRevenue', 'N/A'),
        "profit_margin": info.get('profitMargins', 'N/A')
    }
    
    return {
        "fundamental_analysis": analysis,
        "messages": state["messages"] + ["Fundamental analysis done"]
    }

# === Agent 4: Report Agent ===
def report_agent(state: MultiAgentState) -> MultiAgentState:
    """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
    llm = ChatOpenAI(model="gpt-4")
    
    prompt = f"""
    Generate a comprehensive stock analysis report for {state['ticker']}.
    
    Technical Analysis:
    {state['technical_analysis']}
    
    Fundamental Analysis:
    {state['fundamental_analysis']}
    
    Provide:
    1. Summary
    2. Key insights
    3. Trading recommendation (BUY/HOLD/SELL)
    4. Risk factors
    """
    
    report = llm.invoke(prompt).content
    
    return {
        "final_report": report,
        "messages": state["messages"] + ["Report generated"]
    }

# === æ„å»ºMulti-Agentå·¥ä½œæµ ===
workflow = StateGraph(MultiAgentState)

# æ·»åŠ æ‰€æœ‰agents
workflow.add_node("research", research_agent)
workflow.add_node("technical", technical_agent)
workflow.add_node("fundamental", fundamental_agent)
workflow.add_node("report", report_agent)

# è®¾ç½®æµç¨‹ï¼ˆå¹¶è¡Œåˆ†æï¼‰
workflow.set_entry_point("research")
workflow.add_edge("research", "technical")
workflow.add_edge("research", "fundamental")
workflow.add_edge("technical", "report")
workflow.add_edge("fundamental", "report")
workflow.add_edge("report", END)

# ç¼–è¯‘
app = workflow.compile()

# æµ‹è¯•
if __name__ == "__main__":
    result = app.invoke({
        "ticker": "TSLA",
        "messages": [],
        "research_data": {},
        "technical_analysis": {},
        "fundamental_analysis": {},
        "final_report": ""
    })
    
    print("\n" + "="*60)
    print("å¤šAgentåˆ†ææŠ¥å‘Š")
    print("="*60)
    print(result["final_report"])
```

### Week 4-5äº¤ä»˜ç‰©

**å®Œæˆæ ‡å‡†**:
- âœ… Multi-agentå·¥ä½œæµç³»ç»Ÿ
- âœ… è‡³å°‘3ä¸ªåä½œçš„agents
- âœ… èƒ½è‡ªåŠ¨å®Œæˆï¼šæ•°æ®è·å–â†’åˆ†æâ†’æŠ¥å‘Šç”Ÿæˆ
- âœ… å·¥ä½œæµå¯è§†åŒ–ï¼ˆLangGraphè‡ªå¸¦ï¼‰
- âœ… GitHub repo

**éªŒæ”¶é—®é¢˜**:
1. "å±•ç¤ºä½ çš„Multi-agentç³»ç»Ÿè¿è¡Œè¿‡ç¨‹"
2. "å„ä¸ªAgentå¦‚ä½•åä½œï¼Ÿ"
3. "å¦‚ä½•ç”¨LangGraphæ„å»ºworkflowï¼Ÿ"

---

## ğŸ¯ Week 6: é¡¹ç›®æ•´åˆ

**æœ€ç»ˆé¡¹ç›®**: AIé‡åŒ–äº¤æ˜“ç ”ç©¶ç³»ç»Ÿ

### é¡¹ç›®æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         React Frontend              â”‚
â”‚  - è‚¡ç¥¨æœç´¢ç•Œé¢                      â”‚
â”‚  - Agentå·¥ä½œæµå¯è§†åŒ–                 â”‚
â”‚  - åˆ†ææŠ¥å‘Šå±•ç¤º                      â”‚
â”‚  - RAGé—®ç­”èŠå¤©æ¡†                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“ REST API
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      FastAPI Backend                â”‚
â”‚  - /api/analyze/{ticker}            â”‚
â”‚  - /api/rag/query                   â”‚
â”‚  - /api/sentiment/analyze           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Multi-Agent System              â”‚
â”‚  â”œâ”€ Research Agent                  â”‚
â”‚  â”‚   â””â”€ Tools: yfinance, NewsAPI    â”‚
â”‚  â”œâ”€ Technical Analysis Agent        â”‚
â”‚  â”‚   â””â”€ Tools: TAæŒ‡æ ‡è®¡ç®—            â”‚
â”‚  â”œâ”€ Fundamental Analysis Agent      â”‚
â”‚  â”‚   â””â”€ Tools: è´¢åŠ¡æ•°æ®              â”‚
â”‚  â”œâ”€ RAG Agent                       â”‚
â”‚  â”‚   â””â”€ æŸ¥è¯¢äº¤æ˜“ç¬”è®°çŸ¥è¯†åº“           â”‚
â”‚  â””â”€ Report Agent                    â”‚
â”‚      â””â”€ ç”Ÿæˆç»¼åˆæŠ¥å‘Š                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Day 1-2: åç«¯APIå¼€å‘ï¼ˆ6å°æ—¶ï¼‰

```python
# File: main.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from multi_agent import app as agent_app
from simple_rag import create_rag_system

app = FastAPI(title="AI Stock Research API")

# åˆå§‹åŒ–
rag_system = create_rag_system()

class AnalyzeRequest(BaseModel):
    ticker: str

class RAGQuery(BaseModel):
    question: str

@app.post("/api/analyze/{ticker}")
async def analyze_stock(ticker: str):
    """Multi-agentè‚¡ç¥¨åˆ†æ"""
    try:
        result = agent_app.invoke({
            "ticker": ticker,
            "messages": [],
            "research_data": {},
            "technical_analysis": {},
            "fundamental_analysis": {},
            "final_report": ""
        })
        
        return {
            "ticker": ticker,
            "report": result["final_report"],
            "status": "success"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/rag/query")
async def rag_query(query: RAGQuery):
    """RAGçŸ¥è¯†åº“æŸ¥è¯¢"""
    try:
        answer = rag_system.invoke(query.question)
        return {
            "question": query.question,
            "answer": answer['result'],
            "status": "success"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/")
async def root():
    return {
        "message": "AI Stock Research API",
        "endpoints": [
            "/api/analyze/{ticker}",
            "/api/rag/query"
        ]
    }

# è¿è¡Œ: uvicorn main:app --reload
```

### Day 3-5: å‰ç«¯å¼€å‘ï¼ˆ8å°æ—¶ï¼‰

```typescript
// App.tsx
import { useState } from 'react';
import axios from 'axios';

function App() {
  const [ticker, setTicker] = useState('');
  const [report, setReport] = useState('');
  const [loading, setLoading] = useState(false);

  const analyzeStock = async () => {
    setLoading(true);
    try {
      const response = await axios.post(
        `http://localhost:8000/api/analyze/${ticker}`
      );
      setReport(response.data.report);
    } catch (error) {
      console.error(error);
    }
    setLoading(false);
  };

  return (
    <div className="App">
      <h1>ğŸ“ˆ AI Stock Research</h1>
      
      <div className="search-section">
        <input
          type="text"
          placeholder="Enter ticker (e.g., NVDA)"
          value={ticker}
          onChange={(e) => setTicker(e.target.value.toUpperCase())}
        />
        <button onClick={analyzeStock} disabled={loading}>
          {loading ? 'Analyzing...' : 'Analyze'}
        </button>
      </div>

      {report && (
        <div className="report-section">
          <h2>Analysis Report</h2>
          <pre>{report}</pre>
        </div>
      )}
    </div>
  );
}

export default App;
```

### Day 6-7: é›†æˆæµ‹è¯•å’Œæ–‡æ¡£ï¼ˆ4å°æ—¶ï¼‰

#### é¡¹ç›®READMEæ¨¡æ¿

```markdown
# AI Stock Research System

AI-powered multi-agent system for stock analysis and research.

## Features

- ğŸ¤– Multi-Agent Architecture
  - Research Agent: Data collection
  - Technical Analysis Agent: TA indicators
  - Fundamental Analysis Agent: Financial metrics
  - Report Agent: Comprehensive reports

- ğŸ“š RAG Knowledge Base
  - Query your trading notes
  - Semantic search
  - Context-aware answers

- ğŸ¨ Web Interface
  - Clean React UI
  - Real-time analysis
  - Interactive chat

## Tech Stack

**Backend**:
- Python + FastAPI
- LangGraph (Agent orchestration)
- LangChain (RAG system)
- OpenAI GPT-4

**Frontend**:
- React + TypeScript
- Axios
- Tailwind CSS

**Data**:
- yfinance (Stock data)
- Chroma (Vector database)

## Quick Start

### Backend
```bash
cd backend
pip install -r requirements.txt
uvicorn main:app --reload
```

### Frontend
```bash
cd frontend
npm install
npm start
```

## Demo

[5åˆ†é’Ÿdemoè§†é¢‘é“¾æ¥]

## Architecture

[æ¶æ„å›¾]

## Future Improvements

- [ ] Add more technical indicators
- [ ] Implement backtesting
- [ ] Add risk management module
- [ ] Deploy to cloud

## Author

Jason - [GitHub](https://github.com/yourusername)
```

### Week 6äº¤ä»˜ç‰©

**æœ€ç»ˆé¡¹ç›®åŒ…å«**:
- âœ… å®Œæ•´çš„Multi-agentåç«¯
- âœ… Reactå‰ç«¯ç•Œé¢
- âœ… RAGçŸ¥è¯†åº“é›†æˆ
- âœ… Fine-tunedæ¨¡å‹é›†æˆï¼ˆå¯é€‰ï¼‰
- âœ… å®Œæ•´READMEå’Œæ–‡æ¡£
- âœ… 5åˆ†é’Ÿdemoè§†é¢‘
- âœ… éƒ¨ç½²åˆ°æœ¬åœ°å¯è®¿é—®

**éªŒæ”¶é—®é¢˜**:
1. "æ¼”ç¤ºå®Œæ•´çš„è‚¡ç¥¨åˆ†ææµç¨‹"
2. "è®²è§£ä½ çš„Multi-agentæ¶æ„"
3. "é‡åˆ°äº†å“ªäº›æŠ€æœ¯æŒ‘æˆ˜ï¼Ÿå¦‚ä½•è§£å†³ï¼Ÿ"

---

## ğŸ“š æ ¸å¿ƒèµ„æºæ±‡æ€»

### Week 1-2: RAG

**å¿…çœ‹**:
1. LangChain RAG Tutorial (2å°æ—¶)
   - https://python.langchain.com/docs/tutorials/rag/
   
2. Chroma Quickstart (30åˆ†é’Ÿ)
   - https://docs.trychroma.com/getting-started
   
3. Streamlitæ–‡æ¡£ (1å°æ—¶)
   - https://docs.streamlit.io/get-started

**ä»£ç æ¨¡æ¿**:
- RAGåŸºç¡€æ¨¡æ¿ï¼ˆè§Day 1-2ï¼‰
- Streamlitç•Œé¢æ¨¡æ¿ï¼ˆè§Day 6-7ï¼‰

---

### Week 3: Fine-tuning

**å¿…çœ‹**:
1. HuggingFace Quickstart (1å°æ—¶)
   - https://huggingface.co/docs/transformers/quicktour
   
2. Fine-tuning Guide (2å°æ—¶)
   - https://huggingface.co/docs/transformers/training
   
3. Financial PhraseBankæ•°æ®é›†
   - https://huggingface.co/datasets/financial_phrasebank

**ä»£ç æ¨¡æ¿**:
- Fine-tuningå®Œæ•´è„šæœ¬ï¼ˆè§Day 3-5ï¼‰
- FastAPIéƒ¨ç½²ï¼ˆè§Day 6-7ï¼‰

---

### Week 4-5: Agent

**å¿…çœ‹**:
1. LangChain Quickstart (1å°æ—¶)
   - https://python.langchain.com/docs/get-started/quickstart
   
2. LangGraph Tutorial (3å°æ—¶)
   - https://langchain-ai.github.io/langgraph/tutorials/introduction/
   
3. Tool Calling Guide (1å°æ—¶)
   - https://python.langchain.com/docs/how_to/custom_tools/

**ä»£ç æ¨¡æ¿**:
- LangChainåŸºç¡€ï¼ˆè§Day 1ï¼‰
- LangGraphå·¥ä½œæµï¼ˆè§Day 2-3ï¼‰
- Multi-agentç³»ç»Ÿï¼ˆè§Day 4-7ï¼‰

---

## ğŸ”§ å¿«é€Ÿå‚è€ƒ

### å¸¸ç”¨ä»£ç æ¨¡æ¿

#### RAGæŸ¥è¯¢æ¨¡æ¿

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA

# å¿«é€ŸRAG
def quick_rag(docs_path):
    # åŠ è½½ â†’ åˆ‡åˆ† â†’ åµŒå…¥ â†’ æ£€ç´¢
    loader = DirectoryLoader(docs_path)
    docs = loader.load()
    
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000)
    chunks = splitter.split_documents(docs)
    
    vectorstore = Chroma.from_documents(chunks, OpenAIEmbeddings())
    
    qa = RetrievalQA.from_chain_type(
        llm=ChatOpenAI(),
        retriever=vectorstore.as_retriever()
    )
    
    return qa
```

#### Agentå·¥ä½œæµæ¨¡æ¿

```python
from langgraph.graph import StateGraph, END
from typing import TypedDict

class State(TypedDict):
    input: str
    output: str

def process(state: State) -> State:
    # ä½ çš„é€»è¾‘
    return {"output": f"Processed: {state['input']}"}

# æ„å»ºå·¥ä½œæµ
workflow = StateGraph(State)
workflow.add_node("process", process)
workflow.set_entry_point("process")
workflow.add_edge("process", END)

app = workflow.compile()
result = app.invoke({"input": "test", "output": ""})
```

#### Toolå®šä¹‰æ¨¡æ¿

```python
from langchain.tools import tool

@tool
def your_tool(param: str) -> str:
    """Clear description of what this tool does."""
    # ä½ çš„å®ç°
    result = do_something(param)
    return str(result)
```

---

## ğŸ› å¸¸è§é—®é¢˜é€ŸæŸ¥

### RAGç›¸å…³

**é—®é¢˜**: æ£€ç´¢ä¸å‡†ç¡®
```python
# è§£å†³æ–¹æ¡ˆï¼š
1. è°ƒæ•´chunk_sizeï¼ˆè¯•500, 1000, 1500ï¼‰
2. ä½¿ç”¨Hybrid searchï¼ˆè¯­ä¹‰+å…³é”®è¯ï¼‰
3. æ·»åŠ Re-ranking
```

**é—®é¢˜**: å›ç­”ä¸ç›¸å…³
```python
# è§£å†³æ–¹æ¡ˆï¼š
1. ä¼˜åŒ–Promptæ¨¡æ¿
2. å¢åŠ retrieved documentsæ•°é‡ï¼ˆk=3â†’k=5ï¼‰
3. æ”¹è¿›chunk overlap
```

### Agentç›¸å…³

**é—®é¢˜**: Agenté€‰é”™å·¥å…·
```python
# è§£å†³æ–¹æ¡ˆï¼š
1. ä¼˜åŒ–tool descriptionï¼ˆæ¸…æ™°æè¿°ä½•æ—¶ä½¿ç”¨ï¼‰
2. æ·»åŠ few-shot examples
3. ä½¿ç”¨structured output
```

**é—®é¢˜**: å·¥ä½œæµå¡ä½
```python
# è§£å†³æ–¹æ¡ˆï¼š
1. æ·»åŠ verbose=TrueæŸ¥çœ‹æ—¥å¿—
2. æ£€æŸ¥æ¡ä»¶åˆ¤æ–­é€»è¾‘
3. æ·»åŠ è¶…æ—¶æœºåˆ¶
```

### Fine-tuningç›¸å…³

**é—®é¢˜**: è®­ç»ƒæ…¢
```python
# è§£å†³æ–¹æ¡ˆï¼š
1. å‡å°batch_size
2. ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆdistilbertï¼‰
3. å‡å°‘è®­ç»ƒæ•°æ®é‡
```

**é—®é¢˜**: æ•ˆæœå·®
```python
# è§£å†³æ–¹æ¡ˆï¼š
1. å¢åŠ è®­ç»ƒepochs
2. æ£€æŸ¥æ•°æ®è´¨é‡
3. å°è¯•ä¸åŒçš„å­¦ä¹ ç‡
```

---

## âœ… æ¯å‘¨éªŒæ”¶æ¸…å•

### Week 1-2éªŒæ”¶
- [ ] RAGç³»ç»Ÿèƒ½æ­£ç¡®å›ç­”é—®é¢˜
- [ ] æ”¯æŒPDF/TXTæ–‡æ¡£
- [ ] æœ‰Streamlitç•Œé¢
- [ ] èƒ½å±•ç¤ºç»™åˆ«äººçœ‹

### Week 3éªŒæ”¶
- [ ] Fine-tunedæ¨¡å‹å®Œæˆ
- [ ] å‡†ç¡®ç‡æŠ¥å‘Š
- [ ] éƒ¨ç½²ä¸ºAPI
- [ ] èƒ½è°ƒç”¨æµ‹è¯•

### Week 4-5éªŒæ”¶
- [ ] Multi-agentç³»ç»Ÿè¿è¡Œ
- [ ] è‡³å°‘3ä¸ªagentsåä½œ
- [ ] å·¥ä½œæµæ¸…æ™°å¯è§
- [ ] è¾“å‡ºè´¨é‡å¥½

### Week 6éªŒæ”¶
- [ ] å‰åç«¯é›†æˆå®Œæˆ
- [ ] å®Œæ•´åŠŸèƒ½æ¼”ç¤º
- [ ] READMEæ–‡æ¡£å®Œå–„
- [ ] Demoè§†é¢‘å½•åˆ¶
- [ ] ä»£ç ä¸Šä¼ GitHub

---

## ğŸ“ å­¦ä¹ èŠ‚å¥

### æ¯å¤©æ—¶é—´å®‰æ’ï¼ˆ2-3å°æ—¶ï¼‰

```
7:00-7:30 (30åˆ†é’Ÿ)
â””â”€ çœ‹æ•™ç¨‹/æ–‡æ¡£

7:30-9:00 (1.5å°æ—¶)
â””â”€ å†™ä»£ç å®è·µ

9:00-9:30 (30åˆ†é’Ÿ)
â””â”€ æµ‹è¯•è°ƒè¯•

9:30-10:00 (30åˆ†é’Ÿ)
â””â”€ å†™æ—¥å¿—ï¼Œæäº¤ä»£ç 
```

### å‘¨æœ«å†²åˆºï¼ˆ4-6å°æ—¶ï¼‰

```
ä¸Šåˆ: å®Œæˆæœ¬å‘¨é¡¹ç›®æ ¸å¿ƒåŠŸèƒ½
ä¸‹åˆ: ä¼˜åŒ–ä»£ç ï¼Œæ·»åŠ åŠŸèƒ½
æ™šä¸Š: å†™READMEï¼Œå½•demo
```

---

## ğŸš€ ç«‹åˆ»å¼€å§‹ï¼ˆä»Šæ™šè¡ŒåŠ¨æ¸…å•ï¼‰

### ç¬¬1å°æ—¶: ç¯å¢ƒæ­å»º

```bash
# åˆ›å»ºé¡¹ç›®
mkdir ai-stock-research
cd ai-stock-research

# è™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate

# å®‰è£…ä¾èµ–
pip install langchain langchain-openai langgraph chromadb streamlit

# æµ‹è¯•
python -c "
from langchain_openai import ChatOpenAI
print('âœ… ç¯å¢ƒå‡†å¤‡å®Œæˆ!')
"

# åˆ›å»ºGitHub repo
git init
echo '# AI Stock Research System' > README.md
git add .
git commit -m "Initial commit"
```

### ç¬¬2å°æ—¶: ç¬¬ä¸€ä¸ªRAGåŸå‹

```bash
# å¤åˆ¶"Day 1-2: ç¬¬ä¸€ä¸ªRAGåŸå‹"çš„ä»£ç 
# ä¿å­˜ä¸ºsimple_rag.py
# è¿è¡Œæµ‹è¯•
python simple_rag.py
```

### ä»Šæ™šç»“æŸæ—¶ä½ å·²ç»æœ‰:
- âœ… èƒ½è·‘çš„RAGç³»ç»Ÿ
- âœ… GitHubé¡¹ç›®
- âœ… ç¬¬ä¸€ä¸ªå¯demoçš„åŸå‹

---

## ğŸ“Š è¿›åº¦è¿½è¸ª

### å­¦ä¹ æ—¥å¿—æ¨¡æ¿

```markdown
# AIå­¦ä¹ æ—¥å¿—

## Week 1: RAGç³»ç»Ÿ

### Day 1 - 2025-01-05
**ç”¨æ—¶**: 2å°æ—¶
**å®Œæˆ**:
- [x] ç¯å¢ƒæ­å»º
- [x] ç¬¬ä¸€ä¸ªRAGåŸå‹
- [x] æµ‹è¯•æŸ¥è¯¢åŠŸèƒ½

**é—®é¢˜**:
- Chromaåˆå§‹åŒ–æŠ¥é”™ â†’ å·²è§£å†³ï¼ˆç‰ˆæœ¬é—®é¢˜ï¼‰

**æ˜å¤©è®¡åˆ’**:
- æ·»åŠ PDFæ”¯æŒ
- ä¼˜åŒ–chunk_size
```

---

## ğŸ’¡ æˆåŠŸè¦ç´ 

### å…³é”®åŸåˆ™

```python
âœ… å¿«é€Ÿè¿­ä»£
   - å…ˆè·‘é€šï¼Œå†ä¼˜åŒ–
   - MVPæ€ç»´

âœ… å¤åˆ¶ç²˜è´´
   - ç”¨ç°æˆæ¨¡æ¿
   - ä¸é‡å¤é€ è½®å­

âœ… ç«‹åˆ»å®è·µ
   - çœ‹åˆ°ä»£ç ç«‹åˆ»è¿è¡Œ
   - ä¿®æ”¹å‚æ•°è§‚å¯Ÿæ•ˆæœ

âœ… åŠæ—¶æ±‚åŠ©
   - å¡ä½1å°æ—¶å°±é—®
   - ä¸è¦æ­»ç£•
```

### é¿å…çš„å‘

```
âŒ è¿½æ±‚å®Œç¾ï¼ˆå¤Ÿç”¨å°±è¡Œï¼‰
âŒ æ·±ç©¶åŸç†ï¼ˆå·¥ç¨‹ä¼˜å…ˆï¼‰
âŒ æ”¶è—ä¸ç»ƒï¼ˆç«‹åˆ»åŠ¨æ‰‹ï¼‰
âŒ å­¤å†›å¥‹æˆ˜ï¼ˆåŠæ—¶æ±‚åŠ©ï¼‰
```

---

## ğŸ¯ 6å‘¨åä½ å°†æ‹¥æœ‰

### æŠ€èƒ½æ¸…å•
- âœ… RAGç³»ç»Ÿå¼€å‘
- âœ… æ¨¡å‹Fine-tuning
- âœ… Multi-agentç¼–æ’
- âœ… LangChain/LangGraphç†Ÿç»ƒä½¿ç”¨
- âœ… AIåº”ç”¨å…¨æ ˆå¼€å‘

### é¡¹ç›®ä½œå“
- âœ… RAGçŸ¥è¯†åº“ç³»ç»Ÿ
- âœ… Fine-tunedé‡‘èæ¨¡å‹
- âœ… Multi-agentè‚¡ç¥¨åˆ†æç³»ç»Ÿ
- âœ… å®Œæ•´çš„GitHub portfolio

### é¢è¯•å‡†å¤‡
- âœ… å¯demoçš„å®æˆ˜é¡¹ç›®
- âœ… æŠ€æœ¯æ¶æ„è®²è§£èƒ½åŠ›
- âœ… é—®é¢˜è§£å†³ç»éªŒ
- âœ… AIå·¥ç¨‹åŒ–ç†è§£

---

## ğŸ“ è·å–å¸®åŠ©

**é‡åˆ°é—®é¢˜**:
1. Googleæœç´¢é”™è¯¯ä¿¡æ¯
2. æŸ¥çœ‹å®˜æ–¹æ–‡æ¡£
3. é—®æˆ‘ï¼ˆæä¾›ä»£ç å’Œé”™è¯¯ä¿¡æ¯ï¼‰

**æé—®æ¨¡æ¿**:
```
æˆ‘åœ¨[åšä»€ä¹ˆ]æ—¶é‡åˆ°[ä»€ä¹ˆé”™è¯¯]

ä»£ç :
[minimal reproducible example]

é”™è¯¯ä¿¡æ¯:
[å®Œæ•´error message]

æˆ‘çš„ç†è§£:
[ä½ çš„åˆ†æ]

æˆ‘å°è¯•äº†:
[ä½ è¯•è¿‡çš„æ–¹æ³•]

è¯·é—®å¦‚ä½•è§£å†³?
```

---

## ğŸ‰ å¼€å§‹å§ï¼

**ç°åœ¨ç«‹åˆ»æ‰§è¡Œ**:

```bash
# 5åˆ†é’Ÿåå¼€å§‹ç¬¬ä¸€è¡Œä»£ç 
mkdir ai-stock-research
cd ai-stock-research
python -m venv venv
source venv/bin/activate
pip install langchain langchain-openai langgraph chromadb

# å¼€å§‹Day 1çš„å­¦ä¹ ï¼
```

**6å‘¨åè§è¯ä½ çš„AIé¡¹ç›®ï¼** ğŸš€

---

*æ–‡æ¡£ç‰ˆæœ¬: v1.0*  
*åˆ›å»ºæ—¥æœŸ: 2025-01-05*  
*ç»´æŠ¤è€…: Jason*
